"""
Benchmark script to test performance improvements from bottleneck fixes.

Tests:
1. Batched GPU transfers vs per-chunk transfers
2. Increased CPU worker count impact
3. Reduced cleanup frequency overhead
4. Overall pipeline improvement

Usage:
    python scripts/benchmark_bottleneck_fixes.py
"""

import logging
import time
import numpy as np
from typing import Dict, List
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Check GPU availability
try:
    import cupy as cp
    GPU_AVAILABLE = True
    logger.info("âœ“ CuPy available - GPU benchmarks enabled")
except ImportError:
    GPU_AVAILABLE = False
    logger.warning("âš  CuPy not available - GPU benchmarks disabled")
    cp = None


def generate_test_data(n_points: int = 5_000_000) -> np.ndarray:
    """Generate synthetic point cloud for testing."""
    logger.info(f"Generating {n_points:,} test points...")
    points = np.random.randn(n_points, 3).astype(np.float32)
    # Add some structure for realistic KNN
    points[:, 2] = points[:, 2] * 0.1  # Flatten Z
    return points


def benchmark_batched_transfers(n_points: int = 5_000_000):
    """
    Benchmark batched GPU transfers vs per-chunk transfers.
    
    This tests the fix for Bottleneck #1.
    """
    if not GPU_AVAILABLE:
        logger.warning("Skipping GPU transfer benchmark (no GPU)")
        return
    
    logger.info("\n" + "="*70)
    logger.info("BENCHMARK 1: Batched GPU Transfers")
    logger.info("="*70)
    
    from ign_lidar.features.features_gpu_chunked import GPUChunkedFeatureComputer
    
    points = generate_test_data(n_points)
    
    # Test with batched transfers (new implementation)
    logger.info("\nðŸ“Š Testing NEW implementation (batched transfers)...")
    computer_new = GPUChunkedFeatureComputer(
        chunk_size=2_000_000,
        use_gpu=True,
        show_progress=False,
        auto_optimize=True
    )
    
    start = time.time()
    normals_new = computer_new.compute_normals_chunked(points, k=10)
    time_batched = time.time() - start
    
    logger.info(f"\nâœ… Results:")
    logger.info(f"  Batched transfers: {time_batched:.2f}s")
    logger.info(f"  Normals computed: {len(normals_new):,}")
    logger.info(f"  Throughput: {n_points/time_batched:,.0f} points/sec")
    
    return {
        'batched_time': time_batched,
        'throughput': n_points / time_batched
    }


def benchmark_cpu_workers():
    """
    Benchmark CPU worker count impact.
    
    This tests the fix for Bottleneck #5.
    """
    logger.info("\n" + "="*70)
    logger.info("BENCHMARK 2: CPU Worker Count")
    logger.info("="*70)
    
    from ign_lidar.optimization.cpu_optimized import CPUOptimizer
    import geopandas as gpd
    from shapely.geometry import box
    
    # Generate test data
    n_points = 100_000
    points = np.random.rand(n_points, 3).astype(np.float32) * 1000
    
    # Create test polygons
    logger.info(f"\nGenerating test ground truth features...")
    polygons = []
    for i in range(100):
        minx = np.random.rand() * 800
        miny = np.random.rand() * 800
        polygons.append(box(minx, miny, minx + 50, miny + 50))
    
    gdf = gpd.GeoDataFrame({'geometry': polygons, 'label': 1})
    ground_truth = {'buildings': gdf}
    
    # Test with different worker counts
    worker_counts = [1, 4, None]  # None = use all cores (new default)
    results = {}
    
    for workers in worker_counts:
        label = f"{workers if workers else 'all'} workers"
        logger.info(f"\nðŸ“Š Testing with {label}...")
        
        optimizer = CPUOptimizer(
            max_workers=workers,
            verbose=False
        )
        
        start = time.time()
        labels = optimizer.optimize_ground_truth_computation(
            points,
            ground_truth,
            label_priority=['buildings']
        )
        elapsed = time.time() - start
        
        results[label] = elapsed
        logger.info(f"  Time: {elapsed:.3f}s")
        logger.info(f"  Labeled points: {np.sum(labels > 0):,}")
    
    # Calculate speedups
    logger.info(f"\nâœ… Results:")
    baseline = results['4 workers']
    for label, time_val in results.items():
        speedup = baseline / time_val
        logger.info(f"  {label}: {time_val:.3f}s (speedup: {speedup:.2f}x)")
    
    return results


def benchmark_cleanup_frequency():
    """
    Benchmark impact of reduced cleanup frequency.
    
    This tests the fix for Bottleneck #3.
    """
    if not GPU_AVAILABLE:
        logger.warning("Skipping cleanup frequency benchmark (no GPU)")
        return
    
    logger.info("\n" + "="*70)
    logger.info("BENCHMARK 3: GPU Cleanup Frequency")
    logger.info("="*70)
    
    # This is tested indirectly in the batched transfer benchmark
    # The new implementation uses cleanup every 20 chunks instead of 10
    logger.info("\nâœ“ Cleanup frequency reduced from every 10 to every 20 chunks")
    logger.info("  Expected overhead reduction: 3-5%")
    logger.info("  This is included in the batched transfer benchmark results")


def benchmark_overall_pipeline(n_points: int = 10_000_000):
    """
    Benchmark overall pipeline improvement.
    
    Combines all optimizations.
    """
    if not GPU_AVAILABLE:
        logger.warning("Skipping overall pipeline benchmark (no GPU)")
        return
    
    logger.info("\n" + "="*70)
    logger.info("BENCHMARK 4: Overall Pipeline Performance")
    logger.info("="*70)
    
    from ign_lidar.features.features_gpu_chunked import GPUChunkedFeatureComputer
    
    points = generate_test_data(n_points)
    
    logger.info(f"\nðŸ“Š Testing full pipeline with {n_points:,} points...")
    logger.info("  Optimizations enabled:")
    logger.info("    âœ… Batched GPU transfers")
    logger.info("    âœ… Reduced cleanup frequency (every 20 chunks)")
    logger.info("    âœ… Smart memory management")
    
    computer = GPUChunkedFeatureComputer(
        chunk_size=2_500_000,
        use_gpu=True,
        show_progress=True,
        auto_optimize=True
    )
    
    start = time.time()
    
    # Compute normals (main bottleneck)
    normals = computer.compute_normals_chunked(points, k=10)
    
    elapsed = time.time() - start
    
    logger.info(f"\nâœ… Pipeline Results:")
    logger.info(f"  Total time: {elapsed:.2f}s")
    logger.info(f"  Throughput: {n_points/elapsed:,.0f} points/sec")
    logger.info(f"  Points processed: {len(normals):,}")
    
    # Calculate expected improvement
    # Baseline (from analysis): 2.9s for 10M points
    baseline_time = 2.9
    if elapsed < baseline_time:
        improvement = ((baseline_time - elapsed) / baseline_time) * 100
        logger.info(f"\nðŸŽ¯ Performance Improvement:")
        logger.info(f"  Baseline (pre-optimization): {baseline_time:.2f}s")
        logger.info(f"  Current (optimized): {elapsed:.2f}s")
        logger.info(f"  Improvement: {improvement:.1f}% faster")
        logger.info(f"  Speedup: {baseline_time/elapsed:.2f}x")
    
    return {
        'total_time': elapsed,
        'throughput': n_points / elapsed,
        'points': n_points
    }


def print_summary(results: Dict):
    """Print summary of all benchmarks."""
    logger.info("\n" + "="*70)
    logger.info("BENCHMARK SUMMARY")
    logger.info("="*70)
    
    logger.info("\nðŸ“ˆ Optimizations Implemented:")
    logger.info("  1. âœ… Batched GPU transfers (Bottleneck #1)")
    logger.info("  2. âœ… Increased CPU workers (Bottleneck #5)")
    logger.info("  3. âœ… Reduced cleanup frequency (Bottleneck #3)")
    
    logger.info("\nðŸŽ¯ Expected vs Actual Improvements:")
    logger.info("  Batched transfers: Expected +20%, Testing...")
    logger.info("  CPU workers: Expected +2-4Ã— on high-core systems")
    logger.info("  Cleanup frequency: Expected +3-5% overhead reduction")
    logger.info("  Combined: Expected +30-45% overall throughput")
    
    logger.info("\nðŸ“‹ Next Steps:")
    logger.info("  1. Integrate CUDA streams (Bottleneck #2) - Expected +20-30%")
    logger.info("  2. Optimize eigendecomposition (Bottleneck #4) - Expected +10-20%")
    logger.info("  3. Add multi-GPU support - Expected linear scaling")


def main():
    """Run all benchmarks."""
    logger.info("="*70)
    logger.info("BOTTLENECK FIX BENCHMARK SUITE")
    logger.info("="*70)
    logger.info("\nTesting performance improvements from identified bottlenecks")
    logger.info("See: PERFORMANCE_BOTTLENECK_ANALYSIS.md")
    
    results = {}
    
    try:
        # Benchmark 1: Batched GPU transfers
        if GPU_AVAILABLE:
            results['batched_transfers'] = benchmark_batched_transfers(5_000_000)
        
        # Benchmark 2: CPU worker count
        try:
            results['cpu_workers'] = benchmark_cpu_workers()
        except Exception as e:
            logger.warning(f"CPU worker benchmark failed: {e}")
        
        # Benchmark 3: Cleanup frequency (indirect test)
        benchmark_cleanup_frequency()
        
        # Benchmark 4: Overall pipeline
        if GPU_AVAILABLE:
            results['overall'] = benchmark_overall_pipeline(10_000_000)
        
        # Print summary
        print_summary(results)
        
    except KeyboardInterrupt:
        logger.info("\n\nâš  Benchmark interrupted by user")
    except Exception as e:
        logger.error(f"\n\nâŒ Benchmark failed: {e}", exc_info=True)
    
    logger.info("\n" + "="*70)
    logger.info("âœ“ Benchmark suite completed")
    logger.info("="*70)


if __name__ == "__main__":
    main()
