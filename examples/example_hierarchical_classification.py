"""
Exemple d'utilisation du syst√®me de classification hi√©rarchique am√©lior√©

Ce script montre comment utiliser le nouveau syst√®me de classification multi-niveaux
avec optimisation automatique des seuils, validation et correction d'erreurs.

Fonctionnalit√©s d√©montr√©es:
1. Classification hi√©rarchique (ASPRS -> LOD2 -> LOD3)
2. Utilisation de seuils optimis√©s et adaptatifs
3. Calcul de m√©triques de confiance
4. Validation de la qualit√©
5. Correction automatique des erreurs
6. G√©n√©ration de rapports de qualit√©

Auteur: IGN LiDAR HD Dataset Team
Date: 15 octobre 2025
"""

from pathlib import Path
import logging
import numpy as np
import laspy

# Imports du syst√®me de classification am√©lior√©
from ign_lidar.core.modules.hierarchical_classifier import (
    classify_hierarchical,
    ClassificationLevel,
    HierarchicalClassifier
)
from ign_lidar.core.modules.optimized_thresholds import (
    ClassificationThresholds,
    ClassificationRules
)
from ign_lidar.core.modules.classification_validation import (
    validate_classification,
    auto_correct_classification,
    ClassificationValidator
)

# Imports pour les features
from ign_lidar.features.geometric import compute_geometric_features
from ign_lidar.preprocessing.rgb_augmentation import IGNOrthophotoFetcher
from ign_lidar.preprocessing.infrared_augmentation import IGNInfraredFetcher
from ign_lidar.core.modules.enrichment import compute_ndvi
from ign_lidar.io.wfs_ground_truth import IGNGroundTruthFetcher

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def example_basic_hierarchical_classification():
    """
    Exemple 1: Classification hi√©rarchique basique
    
    D√©montre la classification d'un fichier LAZ du niveau ASPRS vers LOD2
    avec calcul automatique des scores de confiance.
    """
    print("\n" + "=" * 80)
    print("EXEMPLE 1: Classification Hi√©rarchique Basique")
    print("=" * 80 + "\n")
    
    # Charger un fichier LAZ
    input_file = Path("data/test_integration/sample.laz")
    
    if not input_file.exists():
        print(f"‚ö†Ô∏è  Fichier d'exemple non trouv√©: {input_file}")
        print("   Cr√©ez un fichier LAZ de test ou modifiez le chemin")
        return
    
    logger.info(f"üìÇ Chargement: {input_file}")
    las = laspy.read(str(input_file))
    
    # Extraire les labels ASPRS existants
    asprs_labels = np.array(las.classification)
    points = np.vstack([las.x, las.y, las.z]).T
    
    logger.info(f"   {len(points):,} points charg√©s")
    logger.info(f"   Classes ASPRS uniques: {np.unique(asprs_labels)}")
    
    # Classification hi√©rarchique vers LOD2
    logger.info("üîÑ Classification ASPRS -> LOD2...")
    result = classify_hierarchical(
        asprs_labels=asprs_labels,
        target_level='LOD2',
        use_confidence=True,
        track_hierarchy=True
    )
    
    # Afficher les r√©sultats
    stats = result.get_statistics()
    print("\nüìä Statistiques de classification:")
    print(f"   Total points: {stats['total_points']:,}")
    print(f"   Nombre de classes: {stats['num_classes']}")
    print(f"   Confiance moyenne: {stats.get('avg_confidence', 0):.2%}")
    print(f"   Points √† faible confiance: {stats.get('low_confidence_points', 0):,}")
    
    if result.hierarchy_path:
        print("\nüîó Chemin de classification:")
        for step in result.hierarchy_path:
            print(f"   ‚Üí {step}")
    
    print("\nüìà Distribution des classes:")
    for class_id, percentage in sorted(stats['class_percentages'].items()):
        count = stats['class_distribution'][class_id]
        print(f"   Classe {class_id:2d}: {count:8,} points ({percentage:5.1f}%)")
    
    # Sauvegarder les r√©sultats
    output_file = input_file.parent / f"{input_file.stem}_lod2.laz"
    las.classification = result.labels
    las.write(str(output_file))
    logger.info(f"üíæ Sauvegard√©: {output_file}")


def example_advanced_with_features():
    """
    Exemple 2: Classification avanc√©e avec features g√©om√©triques et NDVI
    
    D√©montre l'utilisation de features additionnelles pour raffiner la
    classification et am√©liorer la pr√©cision.
    """
    print("\n" + "=" * 80)
    print("EXEMPLE 2: Classification Avanc√©e avec Features")
    print("=" * 80 + "\n")
    
    # Charger fichier LAZ
    input_file = Path("data/test_integration/sample.laz")
    
    if not input_file.exists():
        print(f"‚ö†Ô∏è  Fichier d'exemple non trouv√©: {input_file}")
        return
    
    logger.info(f"üìÇ Chargement: {input_file}")
    las = laspy.read(str(input_file))
    
    asprs_labels = np.array(las.classification)
    points = np.vstack([las.x, las.y, las.z]).T
    
    # Calculer hauteur au-dessus du sol
    logger.info("üìè Calcul des hauteurs...")
    ground_mask = asprs_labels == 2
    if np.any(ground_mask):
        from scipy.spatial import cKDTree
        ground_points = points[ground_mask]
        tree = cKDTree(ground_points)
        _, nearest_idx = tree.query(points, k=1)
        ground_z = ground_points[nearest_idx, 2]
        height = points[:, 2] - ground_z
    else:
        height = points[:, 2] - points[:, 2].min()
    
    logger.info(f"   Hauteur min: {height.min():.1f}m, max: {height.max():.1f}m")
    
    # Calculer features g√©om√©triques
    logger.info("üîß Calcul des features g√©om√©triques...")
    geom_features = compute_geometric_features(
        points=points,
        k_neighbors=20,
        compute_normals=True,
        compute_planarity=True,
        compute_curvature=True
    )
    
    logger.info(f"   ‚úì Normales, plan√©it√©, courbure calcul√©es")
    
    # Calculer NDVI si RGB et NIR disponibles
    ndvi = None
    if hasattr(las, 'red') and hasattr(las, 'nir'):
        logger.info("üåø Calcul NDVI...")
        red = np.array(las.red, dtype=float)
        nir = np.array(las.nir, dtype=float)
        ndvi = compute_ndvi(red, nir)
        logger.info(f"   NDVI min: {ndvi.min():.2f}, max: {ndvi.max():.2f}")
    else:
        logger.warning("   ‚ö†Ô∏è  RGB/NIR non disponible, NDVI non calcul√©")
    
    # Pr√©parer le dictionnaire de features
    features = {
        'height': height,
        'normals': geom_features.get('normals'),
        'planarity': geom_features.get('planarity'),
        'curvature': geom_features.get('curvature'),
    }
    
    if ndvi is not None:
        features['ndvi'] = ndvi
    
    # Classification avec features
    logger.info("üîÑ Classification avec raffinement par features...")
    result = classify_hierarchical(
        asprs_labels=asprs_labels,
        target_level='LOD2',
        features=features,
        use_confidence=True,
        track_hierarchy=True
    )
    
    # Afficher les r√©sultats
    stats = result.get_statistics()
    print("\nüìä R√©sultats de classification raffin√©e:")
    print(f"   Points raffin√©s: {stats['num_refined']:,}")
    print(f"   Confiance moyenne: {stats.get('avg_confidence', 0):.2%}")
    
    if result.feature_importance:
        print("\nüéØ Importance des features:")
        for feature, importance in sorted(
            result.feature_importance.items(),
            key=lambda x: x[1],
            reverse=True
        ):
            print(f"   {feature:15s}: {importance:.2%}")
    
    # Sauvegarder
    output_file = input_file.parent / f"{input_file.stem}_lod2_advanced.laz"
    las.classification = result.labels
    
    # Ajouter scores de confiance comme extra dimension
    if result.confidence_scores is not None:
        confidence_scaled = (result.confidence_scores * 255).astype(np.uint8)
        # Note: N√©cessite d'ajouter une dimension custom au fichier LAS
        # las.add_extra_dim(laspy.ExtraBytesParams(name="confidence", type=np.uint8))
        # las.confidence = confidence_scaled
    
    las.write(str(output_file))
    logger.info(f"üíæ Sauvegard√©: {output_file}")


def example_adaptive_thresholds():
    """
    Exemple 3: Seuils adaptatifs selon le contexte
    
    D√©montre l'utilisation de seuils adapt√©s au contexte urbain/rural
    et √† la saison.
    """
    print("\n" + "=" * 80)
    print("EXEMPLE 3: Seuils Adaptatifs Selon le Contexte")
    print("=" * 80 + "\n")
    
    # Cr√©er configuration de seuils par d√©faut
    thresholds = ClassificationThresholds()
    
    print("üîß Seuils par d√©faut:")
    print(f"   NDVI v√©g√©tation min: {thresholds.ndvi.vegetation_min:.2f}")
    print(f"   Hauteur sol max: {thresholds.height.ground_max:.2f}m")
    print(f"   Plan√©it√© route min: {thresholds.geometric.planarity_road_min:.2f}")
    
    # Adapter pour contexte urbain + √©t√©
    print("\nüèôÔ∏è  Adaptation pour contexte urbain, saison √©t√©:")
    urban_summer = thresholds.get_adaptive_thresholds(
        season='summer',
        context_type='urban',
        terrain_type='flat'
    )
    
    print(f"   NDVI v√©g√©tation min: {urban_summer.ndvi.vegetation_min:.2f}")
    print(f"   Hauteur sol max: {urban_summer.height.ground_max:.2f}m")
    print(f"   Plan√©it√© route min: {urban_summer.geometric.planarity_road_min:.2f}")
    
    # Adapter pour contexte rural + hiver
    print("\nüå≤ Adaptation pour contexte rural, saison hiver:")
    rural_winter = thresholds.get_adaptive_thresholds(
        season='winter',
        context_type='rural',
        terrain_type='mountainous'
    )
    
    print(f"   NDVI v√©g√©tation min: {rural_winter.ndvi.vegetation_min:.2f}")
    print(f"   Hauteur sol max: {rural_winter.height.ground_max:.2f}m")
    print(f"   Plan√©it√© route min: {rural_winter.geometric.planarity_road_min:.2f}")
    
    # Valider les seuils
    is_valid, warnings = thresholds.validate_thresholds()
    print(f"\n‚úÖ Seuils valides: {is_valid}")
    if warnings:
        print("‚ö†Ô∏è  Avertissements:")
        for warning in warnings:
            print(f"   - {warning}")


def example_validation_and_correction():
    """
    Exemple 4: Validation et correction automatique
    
    D√©montre la validation de la qualit√© de classification et la
    correction automatique des erreurs courantes.
    """
    print("\n" + "=" * 80)
    print("EXEMPLE 4: Validation et Correction Automatique")
    print("=" * 80 + "\n")
    
    # Charger fichier LAZ
    input_file = Path("data/test_integration/sample.laz")
    
    if not input_file.exists():
        print(f"‚ö†Ô∏è  Fichier d'exemple non trouv√©: {input_file}")
        return
    
    logger.info(f"üìÇ Chargement: {input_file}")
    las = laspy.read(str(input_file))
    
    asprs_labels = np.array(las.classification)
    points = np.vstack([las.x, las.y, las.z]).T
    
    # Classification avec features
    # (Code simplifi√© - voir exemple 2 pour version compl√®te)
    height = points[:, 2] - points[:, 2].min()
    
    features = {
        'height': height,
        'points': points
    }
    
    # Classification initiale
    logger.info("üîÑ Classification initiale...")
    result = classify_hierarchical(
        asprs_labels=asprs_labels,
        target_level='LOD2',
        features=features,
        use_confidence=True
    )
    
    # D√©tection d'erreurs
    logger.info("üîç D√©tection des erreurs potentielles...")
    validator = ClassificationValidator()
    errors = validator.detect_errors(
        labels=result.labels,
        features=features,
        confidence_scores=result.confidence_scores
    )
    
    print("\n‚ö†Ô∏è  Erreurs potentielles d√©tect√©es:")
    for error_type, error_mask in errors.items():
        count = np.sum(error_mask)
        percentage = count / len(result.labels) * 100
        print(f"   {error_type:20s}: {count:8,} points ({percentage:5.1f}%)")
    
    # Correction automatique
    logger.info("üîß Correction automatique des erreurs...")
    corrected_labels, correction_counts = auto_correct_classification(
        labels=result.labels,
        features=features,
        confidence_scores=result.confidence_scores,
        confidence_threshold=0.5
    )
    
    print("\n‚úÖ Corrections appliqu√©es:")
    for correction_type, count in correction_counts.items():
        print(f"   {correction_type:15s}: {count:8,} corrections")
    
    # Sauvegarder version corrig√©e
    output_file = input_file.parent / f"{input_file.stem}_lod2_corrected.laz"
    las.classification = corrected_labels
    las.write(str(output_file))
    logger.info(f"üíæ Sauvegard√©: {output_file}")


def example_complete_workflow():
    """
    Exemple 5: Workflow complet de classification
    
    Pipeline complet incluant:
    1. Chargement des donn√©es
    2. Calcul de toutes les features
    3. Classification hi√©rarchique
    4. Validation de la qualit√©
    5. Correction automatique
    6. G√©n√©ration de rapport
    7. Sauvegarde des r√©sultats
    """
    print("\n" + "=" * 80)
    print("EXEMPLE 5: Workflow Complet de Classification")
    print("=" * 80 + "\n")
    
    # Configuration
    input_file = Path("data/test_integration/sample.laz")
    output_dir = Path("data/test_output")
    output_dir.mkdir(exist_ok=True, parents=True)
    
    if not input_file.exists():
        print(f"‚ö†Ô∏è  Fichier d'exemple non trouv√©: {input_file}")
        return
    
    # √âtape 1: Chargement
    logger.info("=" * 60)
    logger.info("√âTAPE 1: Chargement des donn√©es")
    logger.info("=" * 60)
    
    las = laspy.read(str(input_file))
    asprs_labels = np.array(las.classification)
    points = np.vstack([las.x, las.y, las.z]).T
    
    logger.info(f"‚úì {len(points):,} points charg√©s")
    
    # √âtape 2: Calcul des features
    logger.info("\n" + "=" * 60)
    logger.info("√âTAPE 2: Calcul des features")
    logger.info("=" * 60)
    
    # Hauteur
    ground_mask = asprs_labels == 2
    if np.any(ground_mask):
        from scipy.spatial import cKDTree
        ground_points = points[ground_mask]
        tree = cKDTree(ground_points)
        _, nearest_idx = tree.query(points, k=1)
        height = points[:, 2] - ground_points[nearest_idx, 2]
    else:
        height = points[:, 2] - points[:, 2].min()
    
    logger.info(f"‚úì Hauteurs calcul√©es (range: {height.min():.1f} - {height.max():.1f}m)")
    
    # G√©om√©trie
    geom_features = compute_geometric_features(
        points=points,
        k_neighbors=20,
        compute_normals=True,
        compute_planarity=True,
        compute_curvature=True
    )
    logger.info("‚úì Features g√©om√©triques calcul√©es")
    
    features = {
        'height': height,
        'points': points,
        'normals': geom_features.get('normals'),
        'planarity': geom_features.get('planarity'),
        'curvature': geom_features.get('curvature'),
    }
    
    # √âtape 3: Classification
    logger.info("\n" + "=" * 60)
    logger.info("√âTAPE 3: Classification hi√©rarchique")
    logger.info("=" * 60)
    
    result = classify_hierarchical(
        asprs_labels=asprs_labels,
        target_level='LOD2',
        features=features,
        use_confidence=True,
        track_hierarchy=True
    )
    
    stats = result.get_statistics()
    logger.info(f"‚úì Classification termin√©e:")
    logger.info(f"  - {stats['num_classes']} classes d√©tect√©es")
    logger.info(f"  - {stats['num_refined']:,} points raffin√©s")
    logger.info(f"  - Confiance moyenne: {stats.get('avg_confidence', 0):.2%}")
    
    # √âtape 4: Validation
    logger.info("\n" + "=" * 60)
    logger.info("√âTAPE 4: Validation de la qualit√©")
    logger.info("=" * 60)
    
    validator = ClassificationValidator()
    errors = validator.detect_errors(
        labels=result.labels,
        features=features,
        confidence_scores=result.confidence_scores
    )
    
    total_errors = sum(np.sum(mask) for mask in errors.values())
    logger.info(f"‚úì {total_errors:,} erreurs potentielles d√©tect√©es")
    
    # √âtape 5: Correction
    logger.info("\n" + "=" * 60)
    logger.info("√âTAPE 5: Correction automatique")
    logger.info("=" * 60)
    
    corrected_labels, correction_counts = auto_correct_classification(
        labels=result.labels,
        features=features,
        confidence_scores=result.confidence_scores
    )
    
    total_corrections = sum(correction_counts.values())
    logger.info(f"‚úì {total_corrections:,} corrections appliqu√©es")
    
    # √âtape 6: Rapport
    logger.info("\n" + "=" * 60)
    logger.info("√âTAPE 6: G√©n√©ration du rapport")
    logger.info("=" * 60)
    
    report_file = output_dir / f"{input_file.stem}_classification_report.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("RAPPORT DE CLASSIFICATION\n")
        f.write("=" * 80 + "\n\n")
        
        f.write(f"Fichier source: {input_file}\n")
        f.write(f"Date: {stats['total_points']:,} points\n\n")
        
        f.write("STATISTIQUES GLOBALES\n")
        f.write("-" * 80 + "\n")
        f.write(f"Nombre de classes: {stats['num_classes']}\n")
        f.write(f"Points raffin√©s: {stats['num_refined']:,}\n")
        f.write(f"Confiance moyenne: {stats.get('avg_confidence', 0):.2%}\n\n")
        
        f.write("DISTRIBUTION DES CLASSES\n")
        f.write("-" * 80 + "\n")
        for class_id, count in sorted(stats['class_distribution'].items()):
            percentage = stats['class_percentages'][class_id]
            f.write(f"Classe {class_id:2d}: {count:10,} points ({percentage:6.2f}%)\n")
        
        f.write("\n")
        f.write("CORRECTIONS APPLIQU√âES\n")
        f.write("-" * 80 + "\n")
        for corr_type, count in correction_counts.items():
            f.write(f"{corr_type:15s}: {count:10,} corrections\n")
    
    logger.info(f"‚úì Rapport sauvegard√©: {report_file}")
    
    # √âtape 7: Sauvegarde
    logger.info("\n" + "=" * 60)
    logger.info("√âTAPE 7: Sauvegarde des r√©sultats")
    logger.info("=" * 60)
    
    output_file = output_dir / f"{input_file.stem}_lod2_final.laz"
    las.classification = corrected_labels
    las.write(str(output_file))
    
    logger.info(f"‚úì Classification sauvegard√©e: {output_file}")
    logger.info("\nüéâ Workflow complet termin√© avec succ√®s!")


def main():
    """Point d'entr√©e principal."""
    print("\n" + "=" * 80)
    print("EXEMPLES DE CLASSIFICATION HI√âRARCHIQUE AM√âLIOR√âE")
    print("IGN LiDAR HD Dataset - Syst√®me Multi-Niveaux")
    print("=" * 80)
    
    examples = {
        '1': ('Classification hi√©rarchique basique', example_basic_hierarchical_classification),
        '2': ('Classification avanc√©e avec features', example_advanced_with_features),
        '3': ('Seuils adaptatifs contextuels', example_adaptive_thresholds),
        '4': ('Validation et correction automatique', example_validation_and_correction),
        '5': ('Workflow complet', example_complete_workflow),
        'all': ('Tous les exemples', None),
    }
    
    print("\nExemples disponibles:")
    for key, (name, _) in examples.items():
        if key != 'all':
            print(f"  {key}: {name}")
    print(f"  all: Ex√©cuter tous les exemples")
    
    choice = input("\nChoisissez un exemple (1-5 ou 'all', ou 'q' pour quitter): ").strip().lower()
    
    if choice == 'q':
        print("Au revoir!")
        return
    
    if choice == 'all':
        for key in ['1', '2', '3', '4', '5']:
            _, func = examples[key]
            try:
                func()
            except Exception as e:
                logger.error(f"Erreur dans l'exemple {key}: {e}", exc_info=True)
    elif choice in examples and choice != 'all':
        _, func = examples[choice]
        try:
            func()
        except Exception as e:
            logger.error(f"Erreur: {e}", exc_info=True)
    else:
        print(f"Choix invalide: {choice}")
        return
    
    print("\n‚úÖ Termin√©!")


if __name__ == "__main__":
    main()
