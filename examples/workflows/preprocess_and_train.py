#!/usr/bin/env python3
"""
Script de pr√©processing et cr√©ation du dataset d'entra√Ænement.

Ce script:
1. Enrichit les tuiles brutes avec les features g√©om√©triques
2. Cr√©e des patchs de 150m √ó 150m pour l'entra√Ænement

Usage:
    python preprocess_and_train.py
    python preprocess_and_train.py --workers 4
    python preprocess_and_train.py --skip-enrich  # Si d√©j√† enrichi
"""
#!/usr/bin/env python3
"""
DEPRECATED: This script has been moved to examples for reference.

‚ö†Ô∏è  This script is deprecated and kept only for reference.
    Please use the unified CLI instead:
    
    Instead of: python preprocess_and_train.py
    Use: ign-lidar-process enrich [OPTIONS]
    
    See: ign-lidar-process --help
    Or: https://github.com/your-username/ign-lidar-hd#readme

Original script below:
"""


import argparse
import logging
import sys
from pathlib import Path
from datetime import datetime
import json

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def _process_single_tile(args):
    """Worker pour traiter une tuile (niveau module pour pickle)."""
    laz_file, output_dir = args
    
    try:
        import numpy as np
        import laspy
        from ign_lidar.features import compute_all_features_optimized
        
        # 1. Charger le fichier LAZ
        las = laspy.read(str(laz_file))
        
        # 2. Extraire les donn√©es
        points = np.vstack([las.x, las.y, las.z]).T.astype(np.float32)
        classification = np.array(las.classification, dtype=np.uint8)
        
        # 3. Calculer les features
        patch_center = np.mean(points, axis=0)
        
        normals, curvature, height, geo_features = \
            compute_all_features_optimized(
                points=points,
                classification=classification,
                auto_k=True,
                include_extra=True,
                patch_center=patch_center
            )
        
        # 4. Cr√©er le fichier LAZ enrichi
        header = laspy.LasHeader(
            point_format=las.header.point_format,
            version=las.header.version
        )
        header.offsets = las.header.offsets
        header.scales = las.header.scales
        
        # Dimensions suppl√©mentaires
        extra_dims = [
            ("normal_x", "f4"), ("normal_y", "f4"), ("normal_z", "f4"),
            ("curvature", "f4"), ("height_above_ground", "f4"),
            ("density", "f4"), ("planarity", "f4"), ("linearity", "f4"),
            ("sphericity", "f4"), ("anisotropy", "f4"),
            ("roughness", "f4"),
            ("z_normalized", "f4"), ("z_from_ground", "f4"),
            ("verticality", "f4"), ("vertical_std", "f4"),
            ("height_extent_ratio", "f4"),
        ]
        
        las_out = laspy.LasData(header)
        las_out.points = las.points
        
        for dim_name, dim_type in extra_dims:
            las_out.add_extra_dim(laspy.ExtraBytesParams(
                name=dim_name, type=dim_type
            ))
        
        # Remplir les donn√©es
        las_out.normal_x = normals[:, 0].astype(np.float32)
        las_out.normal_y = normals[:, 1].astype(np.float32)
        las_out.normal_z = normals[:, 2].astype(np.float32)
        las_out.curvature = curvature.astype(np.float32)
        las_out.height_above_ground = height.astype(np.float32)
        las_out.density = geo_features['density']
        las_out.planarity = geo_features['planarity']
        las_out.linearity = geo_features['linearity']
        las_out.sphericity = geo_features['sphericity']
        las_out.anisotropy = geo_features['anisotropy']
        las_out.roughness = geo_features['roughness']
        las_out.z_normalized = geo_features['z_normalized']
        las_out.z_from_ground = geo_features['z_from_ground']
        las_out.verticality = geo_features['verticality']
        las_out.vertical_std = geo_features['vertical_std']
        las_out.height_extent_ratio = geo_features['height_extent_ratio']
        
        # Sauvegarder (avec compression LAZ pour compatibilit√© QGIS)
        output_file = output_dir / laz_file.name
        las_out.write(str(output_file), do_compress=True)
        
        return (True, laz_file.name, len(points))
        
    except Exception as e:
        return (False, laz_file.name, str(e))


def _create_patches_for_tile(args):
    """Worker pour cr√©er des patchs d'une tuile (niveau module pour pickle)."""
    laz_file, output_dir, patch_size = args
    
    try:
        import numpy as np
        import laspy
        
        las = laspy.read(str(laz_file))
        x = np.array(las.x)
        y = np.array(las.y)
        
        x_min, x_max = x.min(), x.max()
        y_min, y_max = y.min(), y.max()
        
        nx = int(np.ceil((x_max - x_min) / patch_size))
        ny = int(np.ceil((y_max - y_min) / patch_size))
        
        patch_count = 0
        
        for ix in range(nx):
            for iy in range(ny):
                px_min = x_min + ix * patch_size
                px_max = px_min + patch_size
                py_min = y_min + iy * patch_size
                py_max = py_min + patch_size
                
                mask = ((x >= px_min) & (x < px_max) &
                       (y >= py_min) & (y < py_max))
                
                if not mask.any() or mask.sum() < 1000:
                    continue
                
                patch_las = laspy.LasData(las.header)
                patch_las.points = las.points[mask]
                
                patch_name = (f"{laz_file.stem}_"
                            f"patch_{ix:02d}_{iy:02d}.laz")
                patch_path = output_dir / patch_name
                
                patch_las.write(str(patch_path))
                patch_count += 1
        
        return patch_count
        
    except Exception as e:
        logger.error(f"Erreur {laz_file.name}: {e}")
        return 0


def enrich_tiles(input_dir: Path, output_dir: Path, num_workers: int = 1) -> int:
    """
    Enrichir les tuiles avec toutes les features g√©om√©triques.
    
    Args:
        input_dir: R√©pertoire des tuiles brutes
        output_dir: R√©pertoire des tuiles enrichies
        num_workers: Nombre de workers parall√®les
        
    Returns:
        Nombre de fichiers trait√©s avec succ√®s
    """
    logger.info("=" * 80)
    logger.info("√âTAPE 1/2: ENRICHISSEMENT DES TUILES (MODE BUILDING)")
    logger.info("=" * 80)
    
    import numpy as np
    import laspy
    from ign_lidar.features import compute_all_features_optimized
    import multiprocessing as mp
    from tqdm import tqdm
    
    # Trouver les fichiers LAZ
    laz_files = list(input_dir.rglob("*.laz")) + list(input_dir.rglob("*.LAZ"))
    
    if not laz_files:
        logger.error(f"‚ùå Aucun fichier LAZ trouv√© dans {input_dir}")
        return 0
    
    logger.info(f"üîç Trouv√© {len(laz_files)} fichiers LAZ")
    logger.info(f"üìä Mode: BUILDING (toutes features g√©om√©triques)")
    logger.info(f"‚öôÔ∏è  Workers: {num_workers}")
    logger.info(f"üíæ Sortie: {output_dir}")
    logger.info("")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Pr√©parer les arguments pour les workers
    worker_args = [(laz_file, output_dir) for laz_file in laz_files]
    
    # Traitement parall√®le ou s√©quentiel
    if num_workers > 1:
        logger.info(f"üöÄ Traitement parall√®le avec {num_workers} workers")
        
        with mp.Pool(num_workers) as pool:
            results = list(tqdm(
                pool.imap(_process_single_tile, worker_args),
                total=len(laz_files),
                desc="Enrichissement"
            ))
        
        success_count = sum(1 for r in results if r[0])
        failed_files = [r[1] for r in results if not r[0]]
    else:
        logger.info("üìù Traitement s√©quentiel")
        success_count = 0
        failed_files = []
        
        for i, args in enumerate(worker_args, 1):
            laz_file, _ = args
            logger.info(f"[{i}/{len(laz_files)}] {laz_file.name}")
            success, name, info = _process_single_tile(args)
            
            if success:
                logger.info(f"   ‚úì {info:,} points trait√©s")
                success_count += 1
            else:
                logger.error(f"   ‚úó Erreur: {info}")
                failed_files.append(name)
    
    logger.info(f"\n‚úÖ Enrichissement termin√©:")
    logger.info(f"   {success_count}/{len(laz_files)} fichiers trait√©s")
    
    if failed_files:
        logger.warning(f"   ‚ö†Ô∏è  {len(failed_files)} fichiers √©chou√©s:")
        for f in failed_files[:5]:
            logger.warning(f"      - {f}")
        if len(failed_files) > 5:
            logger.warning(f"      ... et {len(failed_files) - 5} autres")
    
    return success_count


def create_patches(input_dir: Path, output_dir: Path,
                  patch_size: float = 150.0, num_workers: int = 1) -> int:
    """
    Cr√©er des patchs de 150m x 150m.
    
    Args:
        input_dir: R√©pertoire des tuiles enrichies
        output_dir: R√©pertoire des patchs
        patch_size: Taille du patch en m√®tres
        num_workers: Nombre de workers parall√®les
        
    Returns:
        Nombre de patchs cr√©√©s
    """
    logger.info("\n" + "=" * 80)
    logger.info("√âTAPE 2/2: CR√âATION DES PATCHS 150m √ó 150m")
    logger.info("=" * 80)
    
    import numpy as np
    import laspy
    import multiprocessing as mp
    from tqdm import tqdm
    
    laz_files = list(input_dir.glob("*.laz")) + list(input_dir.glob("*.LAZ"))
    
    if not laz_files:
        logger.error(f"‚ùå Aucun fichier LAZ trouv√© dans {input_dir}")
        return 0
    
    logger.info(f"üîç Trouv√© {len(laz_files)} fichiers LAZ enrichis")
    logger.info(f"üìè Taille: {patch_size}m x {patch_size}m "
                f"‚âà {patch_size**2:.1f}m¬≤")
    logger.info(f"‚öôÔ∏è  Workers: {num_workers}")
    logger.info(f"üíæ Sortie: {output_dir}")
    logger.info("")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Pr√©parer les arguments pour les workers
    worker_args = [(laz_file, output_dir, patch_size) for laz_file in laz_files]
    
    # Traitement parall√®le ou s√©quentiel
    if num_workers > 1:
        logger.info(f"üöÄ Traitement parall√®le avec {num_workers} workers")
        
        with mp.Pool(num_workers) as pool:
            patch_counts = list(tqdm(
                pool.imap(_create_patches_for_tile, worker_args),
                total=len(laz_files),
                desc="Cr√©ation patchs"
            ))
        
        total_patches = sum(patch_counts)
    else:
        logger.info("üìù Traitement s√©quentiel")
        total_patches = 0
        
        for i, args in enumerate(worker_args, 1):
            laz_file, _, _ = args
            logger.info(f"[{i}/{len(laz_files)}] {laz_file.name}")
            count = _create_patches_for_tile(args)
            logger.info(f"   ‚úì {count} patchs cr√©√©s")
            total_patches += count
    
    logger.info(f"\n‚úÖ Cr√©ation des patchs termin√©e:")
    logger.info(f"   {total_patches} patchs cr√©√©s au total")
    
    return total_patches


def save_metadata(output_dir: Path, stats: dict):
    """Sauvegarder les m√©tadonn√©es du traitement."""
    metadata = {
        "workflow": "preprocess_and_train",
        "timestamp": datetime.now().isoformat(),
        "configuration": {
            "patch_size_m": 150.0,
            "patch_area_m2": 22500,
            "mode": "building",
            "features": [
                "normal_x", "normal_y", "normal_z",
                "curvature", "height_above_ground",
                "density", "planarity", "linearity",
                "sphericity", "anisotropy", "roughness",
                "z_normalized", "z_from_ground",
                "verticality", "vertical_std", "height_extent_ratio"
            ]
        },
        "statistics": stats
    }
    
    metadata_file = output_dir / "processing_metadata.json"
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    logger.info(f"\nüìÑ M√©tadonn√©es sauvegard√©es: {metadata_file}")


def main():
    """Point d'entr√©e principal."""
    parser = argparse.ArgumentParser(
        description='Pr√©processer les tuiles et cr√©er le dataset entra√Ænement',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  # Traitement complet avec 4 workers
  python preprocess_and_train.py --workers 4
  
  # Sauter l'enrichissement si d√©j√† fait
  python preprocess_and_train.py --skip-enrich --workers 4
  
  # Sauter la cr√©ation des patchs
  python preprocess_and_train.py --skip-patches --workers 4
        """
    )
    
    parser.add_argument(
        '--raw-tiles',
        type=Path,
        default=Path('/mnt/c/Users/Simon/ign/raw_tiles'),
        help='R√©pertoire des tuiles brutes (d√©faut: /mnt/c/Users/Simon/ign/raw_tiles)'
    )
    
    parser.add_argument(
        '--preprocessed',
        type=Path,
        default=Path('/mnt/c/Users/Simon/ign/preprocessed_tiles'),
        help='R√©pertoire des tuiles enrichies (d√©faut: /mnt/c/Users/Simon/ign/preprocessed_tiles)'
    )
    
    parser.add_argument(
        '--dataset',
        type=Path,
        default=Path('dataset/ign_150'),
        help='R√©pertoire du dataset d\'entra√Ænement (d√©faut: dataset/ign_150)'
    )
    
    parser.add_argument(
        '--patch-size',
        type=float,
        default=150.0,
        help='Taille des patchs en m√®tres (d√©faut: 150.0)'
    )
    
    parser.add_argument(
        '--workers',
        type=int,
        default=1,
        help='Nombre de workers parall√®les (d√©faut: 1)'
    )
    
    parser.add_argument(
        '--skip-enrich',
        action='store_true',
        help='Sauter l\'enrichissement (utiliser tuiles d√©j√† enrichies)'
    )
    
    parser.add_argument(
        '--skip-patches',
        action='store_true',
        help='Sauter la cr√©ation des patchs'
    )
    
    args = parser.parse_args()
    
    # V√©rifier que le r√©pertoire source existe
    if not args.raw_tiles.exists():
        logger.error(f"‚ùå R√©pertoire introuvable: {args.raw_tiles}")
        logger.info("Cr√©ez le r√©pertoire ou sp√©cifiez un autre chemin avec --raw-tiles")
        sys.exit(1)
    
    logger.info("=" * 80)
    logger.info("PR√âPROCESSING ET CR√âATION DU DATASET D'ENTRA√éNEMENT")
    logger.info("=" * 80)
    logger.info(f"Configuration:")
    logger.info(f"  - Tuiles brutes:  {args.raw_tiles}")
    logger.info(f"  - Tuiles enrichies: {args.preprocessed}")
    logger.info(f"  - Dataset:         {args.dataset}")
    logger.info(f"  - Patch size:      {args.patch_size}m √ó {args.patch_size}m")
    logger.info(f"  - Workers:         {args.workers}")
    logger.info("")
    
    stats = {}
    
    # √âTAPE 1: Enrichissement
    if not args.skip_enrich:
        enriched_count = enrich_tiles(
            args.raw_tiles,
            args.preprocessed,
            args.workers
        )
        stats['tiles_enriched'] = enriched_count
        
        if enriched_count == 0:
            logger.error("‚ùå Aucune tuile enrichie, arr√™t du traitement")
            sys.exit(1)
    else:
        logger.info("‚è≠Ô∏è  √âtape 1/2: Enrichissement saut√©")
        laz_files = (list(args.preprocessed.glob("*.laz")) +
                    list(args.preprocessed.glob("*.LAZ")))
        stats['tiles_enriched'] = len(laz_files)
        logger.info(f"   {stats['tiles_enriched']} tuiles enrichies trouv√©es")
    
    # √âTAPE 2: Cr√©ation des patchs
    if not args.skip_patches:
        patches_count = create_patches(
            args.preprocessed,
            args.dataset,
            args.patch_size,
            args.workers
        )
        stats['patches_created'] = patches_count
        
        if patches_count == 0:
            logger.error("‚ùå Aucun patch cr√©√©")
            sys.exit(1)
    else:
        logger.info("‚è≠Ô∏è  √âtape 2/2: Cr√©ation des patchs saut√©e")
        laz_files = (list(args.dataset.glob("*.laz")) +
                    list(args.dataset.glob("*.LAZ")))
        stats['patches_created'] = len(laz_files)
    
    # Sauvegarder les m√©tadonn√©es
    save_metadata(args.dataset, stats)
    
    # R√©sum√© final
    logger.info("\n" + "=" * 80)
    logger.info("‚úÖ TRAITEMENT TERMIN√â!")
    logger.info("=" * 80)
    logger.info(f"üìä Statistiques:")
    logger.info(f"   - Tuiles enrichies: {stats.get('tiles_enriched', 'N/A')}")
    logger.info(f"   - Patchs cr√©√©s:     {stats.get('patches_created', 'N/A')}")
    logger.info(f"\nüìÅ R√©pertoires:")
    logger.info(f"   - Tuiles brutes:    {args.raw_tiles}")
    logger.info(f"   - Tuiles enrichies: {args.preprocessed}")
    logger.info(f"   - Dataset training: {args.dataset}")
    logger.info("=" * 80)
    logger.info("")
    logger.info("Prochaines √©tapes:")
    logger.info("  1. V√©rifier les patchs cr√©√©s:")
    logger.info(f"     ls -lh {args.dataset}/*.laz | head")
    logger.info("  2. Utiliser le dataset pour l'entra√Ænement:")
    logger.info(f"     python train.py --data {args.dataset}")


if __name__ == '__main__':
    main()
