# Plan d'EntraÃ®nement LOD2 Auto-SupervisÃ©

## ModÃ¨le Hybride PointNet++ + Point Transformer

**Date:** 15 Octobre 2025  
**Objectif:** Classifier les bÃ¢timents LiDAR HD IGN en LOD2 avec apprentissage auto-supervisÃ©  
**Architecture:** Hybrid PointNet++ + Point Transformer avec index intelligent

---

## ğŸ“‹ Table des MatiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [Architecture du ModÃ¨le](#architecture-du-modÃ¨le)
- [StratÃ©gie Auto-SupervisÃ©e](#stratÃ©gie-auto-supervisÃ©e)
- [Estimation des Ressources](#estimation-des-ressources)
- [Pipeline de DonnÃ©es](#pipeline-de-donnÃ©es)
- [Plan d'EntraÃ®nement](#plan-dentraÃ®nement)
- [ImplÃ©mentation](#implÃ©mentation)
- [Ã‰valuation](#Ã©valuation)

---

## ğŸ¯ Vue d'ensemble

### Objectif Final

DÃ©velopper un modÃ¨le hybride capable de :

1. **PrÃ©-entraÃ®nement auto-supervisÃ©** : Apprendre des reprÃ©sentations gÃ©omÃ©triques sans labels
2. **Classification LOD2** : Distinguer les niveaux de dÃ©tail des bÃ¢timents
3. **GÃ©nÃ©ralisation** : Fonctionner sur diffÃ©rentes rÃ©gions franÃ§aises

### Classes LOD2 (6 niveaux)

```
LOD2.0 : Blocs simples (pavÃ©s)                    [15-20% des bÃ¢timents]
LOD2.1 : Toits simples (1-2 pans)                 [40-45% des bÃ¢timents]
LOD2.2 : Toits complexes (3-4 pans)               [25-30% des bÃ¢timents]
LOD2.3 : Toits trÃ¨s complexes (5+ pans)           [8-10% des bÃ¢timents]
LOD2.4 : Structures spÃ©ciales (dÃ´mes, courbes)    [2-3% des bÃ¢timents]
LOD2.5 : Structures industrielles complexes       [1-2% des bÃ¢timents]
```

---

## ğŸ—ï¸ Architecture du ModÃ¨le

### ModÃ¨le Hybride ProposÃ©

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT POINT CLOUD                         â”‚
â”‚                 (24,576 points, 50m patches)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                           â”‚
         â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PointNet++ SA  â”‚         â”‚ Point Transformer â”‚
â”‚  (Hierarchical) â”‚         â”‚   (Attention)     â”‚
â”‚                 â”‚         â”‚                   â”‚
â”‚  - SA Layer 1   â”‚         â”‚  - Self-Attention â”‚
â”‚  - SA Layer 2   â”‚         â”‚  - Cross-Attentionâ”‚
â”‚  - SA Layer 3   â”‚         â”‚  - Position Enc.  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Fusion Module   â”‚
         â”‚ (Attention Mix) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Classification  â”‚
         â”‚   Head (LOD2)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
            [6 classes LOD2]
```

### Composants ClÃ©s

1. **PointNet++ Branch** (Extraction hiÃ©rarchique)

   - Set Abstraction Layer 1: 2048 points â†’ 512 points
   - Set Abstraction Layer 2: 512 points â†’ 128 points
   - Set Abstraction Layer 3: 128 points â†’ 1 global feature
   - Features: GÃ©omÃ©trie locale, contexte multi-Ã©chelle

2. **Point Transformer Branch** (Attention globale)

   - Self-Attention sur tous les points
   - Positional Encoding 3D
   - Features: Relations long-range, structure globale

3. **Fusion Module** (Attention-based)

   - Cross-attention entre branches
   - Weighted combination
   - Output: Feature vector unifiÃ© (1024-dim)

4. **Index Intelligent** (Spatial Indexing)
   - Octree pour accÃ©lÃ©ration
   - KNN graph prÃ©-calculÃ©
   - Ball Query optimisÃ© GPU

---

## ğŸ”¬ StratÃ©gie Auto-SupervisÃ©e

### Phase 1 : PrÃ©-entraÃ®nement Auto-SupervisÃ© (80% des donnÃ©es)

#### TÃ¢ches Pretext (sans labels)

**1. Reconstruction de Points**

```python
# Masquer 30% des points et les reconstruire
Task: Masked Point Modeling (MPM)
Loss: Chamfer Distance + Earth Mover's Distance
```

**2. PrÃ©diction de Normales**

```python
# PrÃ©dire les normales des surfaces
Task: Normal Vector Prediction
Loss: Cosine Similarity Loss
```

**3. Apprentissage Contrastif**

```python
# DiffÃ©rencier augmentations du mÃªme patch vs patches diffÃ©rents
Task: Contrastive Learning (SimCLR style)
Loss: NT-Xent Loss (Temperature-scaled Cross-Entropy)
```

**4. PrÃ©diction de Rotation**

```python
# PrÃ©dire l'angle de rotation appliquÃ© au patch
Task: Rotation Prediction (0Â°, 90Â°, 180Â°, 270Â°)
Loss: Cross-Entropy (4 classes)
```

**5. PrÃ©diction de Contexte Spatial**

```python
# PrÃ©dire la position relative de patches voisins
Task: Jigsaw Puzzle (9 patches)
Loss: Cross-Entropy (9! permutations â†’ clustered to 100 classes)
```

#### Configuration PrÃ©-entraÃ®nement

```yaml
pretraining:
  epochs: 150
  batch_size: 32
  learning_rate: 0.001
  optimizer: AdamW
  scheduler: CosineAnnealingWarmRestarts

  tasks:
    - name: masked_point_modeling
      weight: 0.3
      mask_ratio: 0.3

    - name: normal_prediction
      weight: 0.2

    - name: contrastive_learning
      weight: 0.3
      temperature: 0.07

    - name: rotation_prediction
      weight: 0.1
      rotations: [0, 90, 180, 270]

    - name: spatial_context
      weight: 0.1
      num_patches: 9
```

### Phase 2 : Fine-tuning SupervisÃ© (20% des donnÃ©es avec labels)

#### Annotation des Labels LOD2

**Option A : Labels Automatiques (RecommandÃ© pour dÃ©marrage)**

```python
# Heuristiques gÃ©omÃ©triques pour labeling initial
def auto_label_lod2(point_cloud):
    """
    GÃ©nÃ¨re des pseudo-labels LOD2 basÃ©s sur:
    - Nombre de plans de toit dÃ©tectÃ©s (RANSAC)
    - ComplexitÃ© gÃ©omÃ©trique (courbure, angles)
    - Variance des normales
    - Volume englobant
    """
    roof_planes = detect_roof_planes(point_cloud)
    complexity = compute_geometric_complexity(point_cloud)

    if roof_planes == 0:
        return LOD2.0  # Bloc simple
    elif roof_planes <= 2:
        return LOD2.1  # Toit simple
    elif roof_planes <= 4:
        return LOD2.2  # Toit complexe
    # ... etc
```

**Option B : Annotation Semi-Automatique**

```bash
# Utiliser l'outil d'annotation fourni
python scripts/annotate_lod2.py \
  --input data/patches_train \
  --output data/patches_labeled \
  --auto-suggest \
  --confidence-threshold 0.8
```

**Option C : Crowd-sourcing / Expert**

- 5000 patches manuellement annotÃ©s par expert (~ 40 heures)
- RÃ©partition Ã©quilibrÃ©e des classes

#### Configuration Fine-tuning

```yaml
finetuning:
  epochs: 50
  batch_size: 16
  learning_rate: 0.0001 # 10x plus faible que prÃ©-entraÃ®nement
  optimizer: AdamW
  scheduler: ReduceLROnPlateau

  loss:
    type: weighted_cross_entropy # Pour gÃ©rer le dÃ©sÃ©quilibre des classes
    weights: [1.0, 1.0, 1.2, 1.5, 2.0, 2.5] # Plus de poids sur classes rares

  data_augmentation:
    - random_rotation: true
    - random_scale: [0.9, 1.1]
    - random_jitter: 0.01
    - point_dropout: 0.05
```

---

## ğŸ’¾ Estimation des Ressources

### Nombre de Tuiles NÃ©cessaires

#### Calcul pour 50m Patches

```
HypothÃ¨ses:
- 1 tuile IGN LiDAR HD = 1km Ã— 1km
- Patch size = 50m Ã— 50m
- Overlap = 15%

Calcul patches par tuile:
- Sans overlap: (1000/50)Â² = 400 patches/tuile
- Avec overlap 15%: ~400 Ã— 1.35 = 540 patches/tuile
- Avec augmentation (5Ã—): 540 Ã— 6 = 3,240 patches/tuile

Pour dataset complet:
- Training (70%): 70,000 patches â†’ ~22 tuiles
- Validation (15%): 15,000 patches â†’ ~5 tuiles
- Test (15%): 15,000 patches â†’ ~5 tuiles

TOTAL: ~32 tuiles minimum
RECOMMANDÃ‰: 50-100 tuiles pour diversitÃ© gÃ©ographique
```

#### Distribution GÃ©ographique RecommandÃ©e

```yaml
dataset_tiles:
  # Phase 1: PrÃ©-entraÃ®nement (40 tuiles)
  pretraining:
    urban_dense: 15 tuiles # Paris, Lyon, Marseille
    urban_medium: 10 tuiles # Villes moyennes
    suburban: 10 tuiles # Zones pÃ©riurbaines
    rural: 5 tuiles # Villages

  # Phase 2: Fine-tuning (10 tuiles)
  finetuning:
    labeled_high_quality: 5 tuiles # Annotations expertes
    labeled_auto: 5 tuiles # Labels automatiques validÃ©s


  # Total: 50 tuiles
```

### Stockage et MÃ©moire

```
Stockage:
- 50m patches avec augmentation: ~4 GB par tuile
- 50 tuiles Ã— 4 GB = 200 GB total
- Checkpoints modÃ¨le: ~500 MB par epoch
- Logs et metadata: ~10 GB

Total stockage: ~250 GB

MÃ©moire GPU (training):
- Batch size 32: ~12 GB VRAM
- RecommandÃ©: RTX 3090 (24GB) ou A100 (40GB)

MÃ©moire GPU (inference):
- Batch size 1: ~2 GB VRAM
- OK pour RTX 3060 (12GB)
```

### Temps de Calcul EstimÃ©

```
Configuration de rÃ©fÃ©rence: NVIDIA RTX 3090 (24GB)

PrÃ©-entraÃ®nement:
- 150 epochs Ã— 70,000 patches
- ~30s per epoch
- Total: ~1.25 heures

Fine-tuning:
- 50 epochs Ã— 20,000 patches
- ~5s per epoch
- Total: ~4 minutes

TOTAL ENTRAÃNEMENT: ~1.5 heures

GÃ©nÃ©ration des patches:
- 50 tuiles Ã— 15 min/tuile
- Total: ~12.5 heures

TOTAL PIPELINE: ~14 heures
```

---

## ğŸ”„ Pipeline de DonnÃ©es

### Ã‰tape 1 : PrÃ©paration des DonnÃ©es (12h)

```bash
# 1. CrÃ©er le dataset multi-Ã©chelle (50m prioritaire pour LOD2)
ign-lidar-hd process \
  experiment=dataset_50m \
  input_dir=/path/to/ign_tiles \
  output_dir=data/lod2_dataset \
  dataset.train_ratio=0.7 \
  dataset.val_ratio=0.15 \
  dataset.test_ratio=0.15 \
  processor.num_workers=4 \
  processor.use_gpu=true

# RÃ©sultat:
# data/lod2_dataset/
#   train/    (~70,000 patches)
#   val/      (~15,000 patches)
#   test/     (~15,000 patches)
#   dataset_metadata.json
```

### Ã‰tape 2 : PrÃ©-entraÃ®nement Auto-SupervisÃ© (1.25h)

```bash
# Script de prÃ©-entraÃ®nement
python train_lod2_selfsupervised.py \
  --mode pretrain \
  --data_dir data/lod2_dataset/train \
  --output_dir models/lod2_pretrained \
  --epochs 150 \
  --batch_size 32 \
  --gpu 0
```

### Ã‰tape 3 : GÃ©nÃ©ration des Pseudo-Labels (30min)

```bash
# Labels automatiques basÃ©s sur gÃ©omÃ©trie
python scripts/generate_lod2_labels.py \
  --input data/lod2_dataset/train \
  --output data/lod2_dataset_labeled \
  --method geometric_heuristics \
  --confidence_threshold 0.8
```

### Ã‰tape 4 : Fine-tuning SupervisÃ© (4min)

```bash
# Fine-tuning avec pseudo-labels
python train_lod2_selfsupervised.py \
  --mode finetune \
  --pretrained_model models/lod2_pretrained/best_model.pth \
  --data_dir data/lod2_dataset_labeled \
  --output_dir models/lod2_finetuned \
  --epochs 50 \
  --batch_size 16 \
  --gpu 0
```

### Ã‰tape 5 : Ã‰valuation (10min)

```bash
# Ã‰valuation sur test set
python evaluate_lod2.py \
  --model models/lod2_finetuned/best_model.pth \
  --data_dir data/lod2_dataset/test \
  --output_dir results/lod2_evaluation
```

---

## ğŸ“Š Plan d'EntraÃ®nement DÃ©taillÃ©

### Semaine 1 : PrÃ©paration des DonnÃ©es

**Jour 1-2 : Collecte et prÃ©traitement**

- [ ] Identifier 50 tuiles IGN reprÃ©sentatives
- [ ] TÃ©lÃ©charger les tuiles via l'API IGN
- [ ] VÃ©rifier la qualitÃ© des donnÃ©es (densitÃ©, couverture)

**Jour 3-4 : GÃ©nÃ©ration des patches**

- [ ] ExÃ©cuter le pipeline de gÃ©nÃ©ration des patches (12h)
- [ ] VÃ©rifier les statistiques du dataset
- [ ] Visualiser des Ã©chantillons de chaque split

**Jour 5 : Validation des donnÃ©es**

- [ ] Analyser la distribution spatiale
- [ ] VÃ©rifier l'absence de leakage entre splits
- [ ] CrÃ©er des visualisations de contrÃ´le qualitÃ©

### Semaine 2 : PrÃ©-entraÃ®nement Auto-SupervisÃ©

**Jour 1 : Configuration**

- [ ] Configurer l'environnement d'entraÃ®nement
- [ ] Tester le chargement des donnÃ©es
- [ ] Valider l'architecture du modÃ¨le

**Jour 2-3 : PrÃ©-entraÃ®nement**

- [ ] Lancer le prÃ©-entraÃ®nement (150 epochs)
- [ ] Monitorer les mÃ©triques de convergence
- [ ] Sauvegarder les checkpoints

**Jour 4 : Analyse du prÃ©-entraÃ®nement**

- [ ] Ã‰valuer la qualitÃ© des features apprises
- [ ] Visualiser les embeddings (t-SNE/UMAP)
- [ ] SÃ©lectionner le meilleur checkpoint

**Jour 5 : GÃ©nÃ©ration des pseudo-labels**

- [ ] Appliquer les heuristiques gÃ©omÃ©triques
- [ ] Valider manuellement un Ã©chantillon (100 patches)
- [ ] Analyser la distribution des classes

### Semaine 3 : Fine-tuning et Ã‰valuation

**Jour 1-2 : Fine-tuning**

- [ ] Fine-tuner sur pseudo-labels (50 epochs)
- [ ] Monitorer l'overfitting
- [ ] Early stopping si nÃ©cessaire

**Jour 3 : Ã‰valuation quantitative**

- [ ] Calculer les mÃ©triques sur test set
- [ ] Analyser la matrice de confusion
- [ ] Identifier les erreurs frÃ©quentes

**Jour 4 : Ã‰valuation qualitative**

- [ ] Visualiser les prÃ©dictions sur cas difficiles
- [ ] Tester sur tuiles hors-distribution
- [ ] Analyser les cas d'Ã©chec

**Jour 5 : Documentation et rapport**

- [ ] Documenter les rÃ©sultats
- [ ] CrÃ©er des visualisations finales
- [ ] PrÃ©parer le rapport technique

### Semaine 4 : Optimisation et DÃ©ploiement

**Jour 1-2 : AmÃ©lioration du modÃ¨le**

- [ ] Annotation manuelle des erreurs majeures
- [ ] Re-fine-tuning avec corrections
- [ ] Optimisation des hyperparamÃ¨tres

**Jour 3 : Export et optimisation**

- [ ] Exporter le modÃ¨le (ONNX, TorchScript)
- [ ] Optimisation pour l'infÃ©rence
- [ ] Tests de performance

**Jour 4-5 : IntÃ©gration**

- [ ] IntÃ©grer le modÃ¨le dans le pipeline IGN
- [ ] CrÃ©er l'API d'infÃ©rence
- [ ] Tester sur production

---

## ğŸ¯ MÃ©triques de SuccÃ¨s

### MÃ©triques de PrÃ©-entraÃ®nement

```python
pretraining_metrics = {
    'masked_point_reconstruction': {
        'chamfer_distance': '< 0.05',  # mÃ¨tres
        'earth_mover_distance': '< 0.08'
    },
    'normal_prediction': {
        'cosine_similarity': '> 0.85',
        'angle_error': '< 15Â°'
    },
    'contrastive_learning': {
        'contrastive_accuracy': '> 0.80',
        'embedding_separation': '> 0.70'
    },
    'rotation_prediction': {
        'accuracy': '> 0.90'
    }
}
```

### MÃ©triques de Classification LOD2

```python
classification_metrics = {
    'global': {
        'overall_accuracy': '> 0.75',  # Target minimum
        'macro_f1_score': '> 0.70',
        'weighted_f1_score': '> 0.75'
    },
    'per_class': {
        'LOD2.0': {'precision': '> 0.80', 'recall': '> 0.75'},
        'LOD2.1': {'precision': '> 0.75', 'recall': '> 0.80'},
        'LOD2.2': {'precision': '> 0.70', 'recall': '> 0.70'},
        'LOD2.3': {'precision': '> 0.65', 'recall': '> 0.60'},
        'LOD2.4': {'precision': '> 0.60', 'recall': '> 0.50'},
        'LOD2.5': {'precision': '> 0.55', 'recall': '> 0.45'}
    }
}
```

---

## ğŸš€ Prochaines Ã‰tapes

### ImmÃ©diat (Cette Semaine)

1. **ExÃ©cuter le script de gÃ©nÃ©ration de configuration**

   ```bash
   cd /mnt/d/Users/Simon/OneDrive/Documents/GitHub/IGN_LIDAR_HD_DATASET
   python scripts/create_lod2_training_pipeline.py
   ```

2. **VÃ©rifier les ressources disponibles**

   - GPU disponible ? (minimum RTX 3060)
   - Espace disque ? (minimum 250 GB)
   - Tuiles IGN accessibles ?

3. **Commencer la gÃ©nÃ©ration des patches**
   ```bash
   ign-lidar-hd process experiment=lod2_selfsupervised \
     input_dir=/path/to/tiles \
     output_dir=data/lod2_training
   ```

### Court Terme (2 Semaines)

1. ImplÃ©menter les tÃ¢ches pretext auto-supervisÃ©es
2. Tester le prÃ©-entraÃ®nement sur petit subset
3. Valider le pipeline complet end-to-end

### Moyen Terme (1 Mois)

1. EntraÃ®nement complet sur 50 tuiles
2. Ã‰valuation et optimisation
3. Documentation des rÃ©sultats

---

## ğŸ“š RÃ©fÃ©rences

- **PointNet++**: Qi et al. "PointNet++: Deep Hierarchical Feature Learning on Point Sets" (2017)
- **Point Transformer**: Zhao et al. "Point Transformer" (2021)
- **Self-Supervised Learning**: He et al. "Momentum Contrast for Unsupervised Visual Representation Learning" (2020)
- **Masked Autoencoders**: Pang et al. "Masked Autoencoders for Point Cloud Self-supervised Learning" (2022)

---

## ğŸ“ Support

Pour toute question sur ce plan d'entraÃ®nement :

- Consulter la documentation complÃ¨te : `docs/ML_DATASET_CREATION.md`
- VÃ©rifier les exemples : `examples/MULTI_SCALE_TRAINING_STRATEGY.md`
- Ouvrir une issue sur GitHub
