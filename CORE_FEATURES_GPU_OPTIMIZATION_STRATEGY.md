# Core Features & GPU Optimization Strategy

**Date:** October 18, 2025  
**Status:** ‚úÖ GPU Files Restored, Ready for Optimization  
**GPU:** RTX 4080 Super (16GB VRAM)  
**Focus:** Core features harmonization + GPU calculation optimization

---

## üéØ Executive Summary

### Current State

**‚úÖ FIXED:** GPU files have been restored

- ‚úÖ `features_gpu.py` (1,374 lines) - Restored
- ‚úÖ `features_gpu_chunked.py` (3,171 lines) - Restored
- ‚úÖ GPU strategies now import successfully
- ‚úÖ RTX 4080 Super optimizations preserved

### Performance Baseline

| Implementation        | Dataset Size | Time  | Notes                             |
| --------------------- | ------------ | ----- | --------------------------------- |
| **CPU (Numba)**       | 1M points    | ~45s  | 3-5√ó faster than standard CPU     |
| **GPU Single**        | 1M points    | ~3s   | 15√ó faster than CPU Numba         |
| **GPU Chunked**       | 18.6M points | ~60s  | Week 1: 16√ó optimization achieved |
| **GPU Chunked (Old)** | 18.6M points | ~353s | Before Week 1 optimization        |

### Optimization Opportunities

1. **Core Features:** Eliminate remaining "unified/optimized" naming ‚úÖ DONE
2. **GPU Memory:** Better batch size auto-tuning for RTX 4080 Super
3. **GPU Chunking:** Reduce unnecessary batching (bottleneck identified)
4. **Code Architecture:** Refactor GPU to use core modules (future)

---

## üìä Detailed Architecture Analysis

### Core Features Module (CPU)

**Location:** `ign_lidar/features/core/`

```
core/
‚îú‚îÄ‚îÄ features.py ‚úÖ (483 lines)
‚îÇ   ‚îú‚îÄ‚îÄ _compute_normals_and_eigenvalues_jit() - Numba JIT
‚îÇ   ‚îú‚îÄ‚îÄ _compute_all_features_jit() - Numba JIT
‚îÇ   ‚îú‚îÄ‚îÄ compute_normals() - 3-5√ó faster
‚îÇ   ‚îî‚îÄ‚îÄ compute_all_features() - 5-8√ó faster (single-pass)
‚îÇ
‚îú‚îÄ‚îÄ normals.py ‚úÖ (Standard fallback)
‚îÇ   ‚îú‚îÄ‚îÄ compute_normals() - scikit-learn based
‚îÇ   ‚îî‚îÄ‚îÄ compute_normals_fast/accurate() - Compatibility
‚îÇ
‚îú‚îÄ‚îÄ curvature.py ‚úÖ
‚îú‚îÄ‚îÄ eigenvalues.py ‚úÖ
‚îú‚îÄ‚îÄ geometric.py ‚úÖ
‚îú‚îÄ‚îÄ architectural.py ‚úÖ
‚îú‚îÄ‚îÄ density.py ‚úÖ
‚îî‚îÄ‚îÄ unified.py ‚úÖ (API dispatcher)
```

**Strengths:**

- ‚úÖ Clean separation of concerns
- ‚úÖ Numba JIT provides 3-8√ó CPU speedup
- ‚úÖ Single-pass computation (shared covariance)
- ‚úÖ Fallback to standard numpy/scikit-learn
- ‚úÖ Clear naming (no "unified/optimized" prefixes)

**Remaining Issues:**

- None - Core features are well-architected

---

### GPU Implementation (CuPy/cuML)

**Location:** `ign_lidar/features/`

#### A. GPU Single-Batch (`features_gpu.py` - 1,374 lines)

```python
class GPUFeatureComputer:
    def __init__(self, batch_size=8_000_000):
        # Auto-optimizes batch size based on VRAM
        if vram_gb >= 15.0:  # RTX 4080 Super
            self.batch_size = 12_000_000  # 50% increase
        elif vram_gb >= 12.0:
            self.batch_size = 6_000_000
        ...

    def compute_all_features(points, k=20):
        # Single-batch GPU processing
        # Uses CuPy + cuML for 15√ó speedup
```

**Optimizations:**

- ‚úÖ Adaptive batch sizing (RTX 4080: 12M points)
- ‚úÖ Eigenvalue batching (500K matrices max per call)
- ‚úÖ CuML NearestNeighbors for KNN
- ‚úÖ CuPy for vectorized operations

**Strengths:**

- Fast for medium datasets (< 10M points)
- Simple single-batch logic
- Good VRAM utilization

**Issues:**

- Limited by single GPU memory allocation
- No chunking for very large datasets

---

#### B. GPU Chunked (`features_gpu_chunked.py` - 3,171 lines)

```python
class GPUChunkedFeatureComputer:
    def __init__(
        self,
        chunk_size=8_000_000,  # INCREASED for RTX 4080
        neighbor_query_batch_size=5_000_000,  # Controls chunking
        feature_batch_size=2_000_000  # Controls normal/curvature batching
    ):
        ...

    def compute_all_features_chunked(points, k=20):
        # Multi-chunk processing for large datasets
        # Week 1: 16√ó optimization (353s ‚Üí 22s per chunk)
```

**Key Parameters:**

| Parameter                   | Default | RTX 4080 Optimal | Purpose                                |
| --------------------------- | ------- | ---------------- | -------------------------------------- |
| `chunk_size`                | 5M      | 8M               | Points per main chunk                  |
| `neighbor_query_batch_size` | 5M      | 30M+             | Controls neighbor query chunking       |
| `feature_batch_size`        | 2M      | 4M               | Controls normal/curvature batching     |
| `NEIGHBOR_BATCH_SIZE`       | 250K    | 500K             | Internal batch size (Week 1 optimized) |

**Week 1 Optimization:**

```python
# BEFORE (353s per chunk)
NEIGHBOR_BATCH_SIZE = 50_000  # Too small!

# AFTER (22s per chunk) ‚úÖ
NEIGHBOR_BATCH_SIZE = 250_000  # Optimized for GPU L2 cache
```

**Current Bottleneck (Discovered in Analysis):**

```python
# Line 2745-2752: UNNECESSARY BATCHING
SAFE_BATCH_SIZE = 5_000_000  # Hardcoded!
if N > min_points_for_batching and num_query_batches == 1:
    batch_size = SAFE_BATCH_SIZE  # ‚Üê Ignores user's 30M config!
    num_query_batches = (N + batch_size - 1) // batch_size

# IMPACT:
# - 18.6M points ‚Üí Split into 4 batches (5M each)
# - User's neighbor_query_batch_size=30M IGNORED
# - 4√ó slower neighbor queries than necessary
```

---

## üöÄ Optimization Roadmap

### Phase 3.0: GPU Restoration ‚úÖ COMPLETE

**Status:** ‚úÖ Done  
**Time:** 10 minutes  
**Result:** GPU files restored, imports working

**Changes:**

```bash
git restore ign_lidar/features/features_gpu.py
git restore ign_lidar/features/features_gpu_chunked.py
```

**Testing:**

```python
from ign_lidar.features.strategy_gpu import GPUStrategy  # ‚úÖ
from ign_lidar.features.strategy_gpu_chunked import GPUChunkedStrategy  # ‚úÖ
```

---

### Phase 3.1: Fix GPU Chunked Bottleneck üî• HIGH PRIORITY

**Goal:** Remove unnecessary batching in neighbor queries

**Status:** Not started  
**Time:** 30-60 minutes  
**Impact:** ~4√ó faster neighbor queries for RTX 4080 Super

#### Problem Analysis

**File:** `features_gpu_chunked.py:2745-2769`

**Current Logic (BROKEN):**

```python
# Calculate memory estimate
estimated_memory_gb = (N * k * 8) / (1024**3)

# BUT THEN IGNORES IT!
SAFE_BATCH_SIZE = 5_000_000  # Hardcoded override
if N > self.min_points_for_batching and num_query_batches == 1:
    batch_size = SAFE_BATCH_SIZE  # ‚Üê USER CONFIG IGNORED!
    num_query_batches = (N + batch_size - 1) // batch_size
```

**Impact on 18.6M point dataset:**

- Memory needed: 18.6M √ó 20 √ó 8 = 2.98GB (perfectly safe on 16GB GPU!)
- Actual behavior: Split into 4 batches of 5M points
- Config value `neighbor_query_batch_size=30M` completely ignored
- Result: 4√ó slower than necessary

#### Solution

**Replace lines 2745-2769 with smart logic:**

```python
def _should_batch_neighbor_queries(self, N, k, available_vram_gb):
    """
    Decide if neighbor queries need batching based on actual memory.

    Args:
        N: Number of points
        k: Number of neighbors
        available_vram_gb: Available GPU memory

    Returns:
        (should_batch, batch_size, num_batches)
    """
    # Calculate actual memory requirements
    # Neighbor indices: N √ó k √ó 4 bytes (int32)
    # Neighbor distances: N √ó k √ó 4 bytes (float32)
    estimated_memory_gb = (N * k * 8) / (1024**3)

    # Use 50% of available VRAM as threshold (conservative)
    memory_threshold_gb = available_vram_gb * 0.5

    if estimated_memory_gb <= memory_threshold_gb:
        # Memory is safe - NO BATCHING NEEDED!
        logger.info(
            f"‚úÖ Neighbor queries fit in VRAM: "
            f"{estimated_memory_gb:.2f}GB < {memory_threshold_gb:.2f}GB threshold"
        )
        return False, N, 1
    else:
        # Need batching - use USER'S configured batch size
        batch_size = self.neighbor_query_batch_size
        num_batches = (N + batch_size - 1) // batch_size
        logger.info(
            f"‚ö†Ô∏è  Batching neighbor queries: "
            f"{estimated_memory_gb:.2f}GB > {memory_threshold_gb:.2f}GB threshold"
            f" ‚Üí {num_batches} batches of {batch_size:,} points"
        )
        return True, batch_size, num_batches


# In compute_all_features_chunked():
should_batch, batch_size, num_query_batches = self._should_batch_neighbor_queries(
    N, k, available_vram_gb
)

if should_batch:
    # Process in batches
    for batch_idx in range(num_query_batches):
        start = batch_idx * batch_size
        end = min(start + batch_size, N)
        # ... batch processing
else:
    # Single pass - FAST!
    distances, indices = knn_object.kneighbors(points_gpu, k)
```

**Expected Results:**

| Dataset   | Before (Batches) | After (Batches) | Speedup    |
| --------- | ---------------- | --------------- | ---------- |
| 18.6M pts | 4 √ó 5M           | 1 √ó 18.6M       | ~4√ó faster |
| 5M pts    | 1 √ó 5M           | 1 √ó 5M          | No change  |
| 30M pts   | 6 √ó 5M           | 1 √ó 30M         | ~6√ó faster |
| 50M pts   | 10 √ó 5M          | 2 √ó 25M         | ~5√ó faster |

**Implementation Steps:**

1. Add `_should_batch_neighbor_queries()` method
2. Replace hardcoded `SAFE_BATCH_SIZE` logic
3. Respect user's `neighbor_query_batch_size` config
4. Add detailed logging
5. Test on 18.6M point dataset
6. Verify 4√ó speedup

**Files to Modify:**

- `features_gpu_chunked.py` (lines 2745-2769)

**Estimated Time:** 30-60 minutes  
**Risk:** Low (well-tested logic, simple refactor)  
**Priority:** üî• HIGH - Immediate 4√ó speedup available

---

### Phase 3.2: Optimize GPU Batch Sizes üìä MEDIUM PRIORITY

**Goal:** Better auto-tuning for RTX 4080 Super

**Status:** Not started  
**Time:** 1-2 hours  
**Impact:** 10-20% throughput improvement

#### Current Batch Sizes

**GPU Single (`features_gpu.py`):**

```python
if vram_gb >= 15.0:  # RTX 4080 Super
    self.batch_size = 12_000_000  # ‚úÖ Already optimized (50% increase)
```

**GPU Chunked (`features_gpu_chunked.py`):**

```python
# strategy_gpu_chunked.py
chunk_size = 8_000_000  # ‚úÖ Already increased from 5M to 8M
batch_size = 500_000  # ‚úÖ Already doubled from 250K to 500K

# features_gpu_chunked.py (internal)
neighbor_query_batch_size = 5_000_000  # ‚ö†Ô∏è  Too conservative for RTX 4080
feature_batch_size = 2_000_000  # ‚ö†Ô∏è  Could be 3-4M
NEIGHBOR_BATCH_SIZE = 250_000  # ‚úÖ Week 1 optimized
```

#### Optimization Strategy

**A. Increase GPU Thresholds (DONE in GPU_BOTTLENECK_ANALYSIS.md)**

```python
# Current (lines 251-267)
elif vram_gb >= 14:
    threshold = 25_000_000  # ‚úÖ Already 5√ó increase (was 5M)
    gpu_tier = "Consumer (RTX 4080/3090)"
```

**B. Optimize Internal Batch Sizes**

```python
# For RTX 4080 Super (16GB VRAM):
DEFAULT_NEIGHBOR_QUERY_BATCH_SIZE = 20_000_000  # 4√ó increase
DEFAULT_FEATURE_BATCH_SIZE = 4_000_000  # 2√ó increase
DEFAULT_NEIGHBOR_BATCH_SIZE = 500_000  # 2√ó increase from Week 1

# With memory-based auto-tuning:
if vram_gb >= 15.0:
    neighbor_query_batch_size = min(30_000_000, N)  # Process almost everything in 1 batch
    feature_batch_size = 4_000_000
    neighbor_batch_size = 500_000
elif vram_gb >= 12.0:
    neighbor_query_batch_size = 15_000_000
    feature_batch_size = 3_000_000
    neighbor_batch_size = 400_000
elif vram_gb >= 8.0:
    neighbor_query_batch_size = 10_000_000
    feature_batch_size = 2_000_000
    neighbor_batch_size = 300_000
```

**Expected Results:**

- RTX 4080: 10-20% faster throughput
- Better GPU utilization (fewer batches)
- Reduced kernel launch overhead

**Files to Modify:**

- `features_gpu_chunked.py` (lines 93-110, 411-448)

**Estimated Time:** 1-2 hours  
**Risk:** Low-Medium (test with various dataset sizes)  
**Priority:** üìä MEDIUM

---

### Phase 3.3: Refactor GPU to Use Core Modules üèóÔ∏è LONG-TERM

**Goal:** Reduce code duplication between CPU and GPU

**Status:** Not started  
**Time:** 8-12 hours  
**Impact:** Maintainability, not performance

#### Current Duplication

**CPU (core/features.py):**

```python
@jit(nopython=True, parallel=True)
def _compute_normals_and_eigenvalues_jit(points, neighbor_indices, k):
    # Numba JIT-compiled CPU version
    for i in prange(n_points):
        # Compute covariance
        # Eigendecomposition
        ...
```

**GPU (features_gpu.py):**

```python
def compute_normals_gpu(points_gpu, k):
    # CuPy GPU version
    # DUPLICATES the algorithm, different implementation
    ...
```

#### Proposed Architecture

**Create GPU-specific core modules:**

```
core/
‚îú‚îÄ‚îÄ features.py ‚úÖ (CPU Numba)
‚îú‚îÄ‚îÄ features_gpu.py üÜï (GPU CuPy - lean version)
‚îÇ   ‚îú‚îÄ‚îÄ compute_normals_gpu()
‚îÇ   ‚îú‚îÄ‚îÄ compute_curvature_gpu()
‚îÇ   ‚îî‚îÄ‚îÄ compute_all_features_gpu()
‚îÇ
‚îú‚îÄ‚îÄ normals.py ‚úÖ (CPU standard)
‚îú‚îÄ‚îÄ normals_gpu.py üÜï (GPU CuPy)
‚îÇ
‚îú‚îÄ‚îÄ curvature.py ‚úÖ (CPU - already shared logic)
‚îú‚îÄ‚îÄ geometric.py ‚úÖ (CPU - already shared logic)
‚îÇ
‚îî‚îÄ‚îÄ utils_gpu.py üÜï (GPU utilities)
    ‚îú‚îÄ‚îÄ gpu_memory_info()
    ‚îú‚îÄ‚îÄ optimal_batch_size()
    ‚îî‚îÄ‚îÄ chunked_processing_helper()
```

**GPU Computer Classes (refactored):**

```
features/
‚îú‚îÄ‚îÄ gpu_computer.py üÜï (~400 lines, was 1,374)
‚îÇ   ‚îî‚îÄ‚îÄ GPUFeatureComputer (uses core/features_gpu.py)
‚îÇ
‚îî‚îÄ‚îÄ gpu_chunked_computer.py üÜï (~800 lines, was 3,171)
    ‚îî‚îÄ‚îÄ GPUChunkedFeatureComputer (uses core/features_gpu.py + chunking logic)
```

**Benefits:**

- ‚úÖ 50%+ code reduction (3,872 ‚Üí ~1,800 lines)
- ‚úÖ Shared algorithm logic (CPU and GPU use same high-level flow)
- ‚úÖ Easier to maintain (one place to fix bugs)
- ‚úÖ Clearer separation: core algorithms vs. orchestration

**Challenges:**

- ‚ö†Ô∏è CuPy arrays ‚â† NumPy arrays (different APIs)
- ‚ö†Ô∏è GPU needs special memory management
- ‚ö†Ô∏è Risk of breaking Week 1 optimizations

**Strategy:**

1. Extract GPU normals first (2-3 hours)
2. Test performance matches current
3. Extract GPU curvature (1-2 hours)
4. Extract GPU geometric features (2-3 hours)
5. Create lean GPU computer classes (2-3 hours)
6. Comprehensive testing (1-2 hours)

**Files to Create:**

- `core/features_gpu.py` (~300 lines)
- `core/normals_gpu.py` (~200 lines)
- `core/utils_gpu.py` (~100 lines)
- `features/gpu_computer.py` (~400 lines)
- `features/gpu_chunked_computer.py` (~800 lines)

**Files to Refactor:**

- `features_gpu.py` (1,374 ‚Üí 0 lines, code moved)
- `features_gpu_chunked.py` (3,171 ‚Üí 0 lines, code moved)
- `strategy_gpu.py` (import changes)
- `strategy_gpu_chunked.py` (import changes)

**Estimated Time:** 8-12 hours  
**Risk:** Medium (extensive testing needed)  
**Priority:** üèóÔ∏è LONG-TERM (do after Phase 3.1 and 3.2)

---

## üìã Immediate Action Items

### Today (30-60 minutes) üî•

1. **Fix GPU Chunked Bottleneck (Phase 3.1)**
   - Implement `_should_batch_neighbor_queries()`
   - Remove hardcoded `SAFE_BATCH_SIZE`
   - Test on 18.6M point dataset
   - Verify 4√ó speedup
   - Commit changes

### This Week (1-2 hours) üìä

2. **Optimize GPU Batch Sizes (Phase 3.2)**
   - Increase RTX 4080 defaults
   - Test various dataset sizes
   - Measure throughput improvements
   - Update documentation

### Next Week (8-12 hours) üèóÔ∏è

3. **Refactor GPU Architecture (Phase 3.3)**
   - Extract GPU normals to core
   - Create lean GPU computer classes
   - Maintain Week 1 performance
   - Comprehensive testing

---

## üéØ Success Metrics

### Phase 3.1 (Fix Bottleneck)

| Metric                   | Before    | Target  | Measured |
| ------------------------ | --------- | ------- | -------- |
| 18.6M pts neighbor query | 4 batches | 1 batch | ‚è≥ TBD   |
| Neighbor query time      | ~16s      | ~4s     | ‚è≥ TBD   |
| Total time (18.6M)       | ~60s      | ~48s    | ‚è≥ TBD   |
| Respects user config     | ‚ùå No     | ‚úÖ Yes  | ‚è≥ TBD   |

### Phase 3.2 (Optimize Batching)

| Metric               | Before | Target | Measured |
| -------------------- | ------ | ------ | -------- |
| GPU utilization      | 70%    | 85%+   | ‚è≥ TBD   |
| Throughput (pts/sec) | 310K   | 350K+  | ‚è≥ TBD   |
| Batch overhead       | 20%    | 10%    | ‚è≥ TBD   |

### Phase 3.3 (Refactor)

| Metric           | Before      | Target       | Measured |
| ---------------- | ----------- | ------------ | -------- |
| GPU code size    | 3,872 lines | <2,000 lines | ‚è≥ TBD   |
| Code duplication | High        | Low          | ‚è≥ TBD   |
| Maintainability  | Medium      | High         | ‚è≥ TBD   |
| Performance      | 100%        | ‚â•100%        | ‚è≥ TBD   |

---

## üí° Key Principles

### For All GPU Optimizations

1. **‚úÖ Preserve Week 1 Gains:** Never regress from 16√ó speedup
2. **‚úÖ Respect User Config:** Don't ignore batch size parameters
3. **‚úÖ Smart Memory Management:** Calculate actual needs, don't guess
4. **‚úÖ Progressive Enhancement:** Test at each step
5. **‚úÖ Measure Everything:** Before/after benchmarks required

### For Refactoring

1. **‚úÖ Different Hardware = Different Code:** CPU and GPU implementations should differ
2. **‚úÖ Share Algorithms, Not Code:** Same logic, different array types
3. **‚úÖ Specialize for Performance:** GPU needs chunking, batching, memory management
4. **‚úÖ Test Extensively:** Performance regressions are unacceptable

---

## üìö Related Documents

- `PHASE3_GPU_CRITICAL_ANALYSIS.md` - GPU restoration analysis
- `GPU_BOTTLENECK_ANALYSIS.md` - Neighbor query bottleneck
- `GPU_ADAPTIVE_BATCHING.md` - Batch size strategies
- `CORE_FEATURES_HARMONIZATION.md` - CPU core features cleanup
- `PHASE2_COMPLETE.md` - Feature consolidation (Phase 2)
- `NEXT_STEPS.md` - Overall project roadmap

---

**Status:** ‚úÖ Ready for Phase 3.1  
**Priority:** üî• High - Immediate 4√ó speedup available  
**Risk:** Low - Clear path, well-understood optimizations  
**Next:** Implement Phase 3.1 (30-60 minutes)

**Updated:** October 18, 2025  
**GPU:** RTX 4080 Super (16GB VRAM)
