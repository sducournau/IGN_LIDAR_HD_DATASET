# Core Features & GPU Optimization Strategy

**Date:** October 18, 2025  
**Status:** âœ… GPU Files Restored, Ready for Optimization  
**GPU:** RTX 4080 Super (16GB VRAM)  
**Focus:** Core features harmonization + GPU calculation optimization

---

## ğŸ¯ Executive Summary

### Current State

**âœ… FIXED:** GPU files have been restored

- âœ… `features_gpu.py` (1,374 lines) - Restored
- âœ… `features_gpu_chunked.py` (3,171 lines) - Restored
- âœ… GPU strategies now import successfully
- âœ… RTX 4080 Super optimizations preserved

### Performance Baseline

| Implementation        | Dataset Size | Time  | Notes                             |
| --------------------- | ------------ | ----- | --------------------------------- |
| **CPU (Numba)**       | 1M points    | ~45s  | 3-5Ã— faster than standard CPU     |
| **GPU Single**        | 1M points    | ~3s   | 15Ã— faster than CPU Numba         |
| **GPU Chunked**       | 18.6M points | ~60s  | Week 1: 16Ã— optimization achieved |
| **GPU Chunked (Old)** | 18.6M points | ~353s | Before Week 1 optimization        |

### Optimization Opportunities

1. **Core Features:** Eliminate remaining "unified/optimized" naming âœ… DONE
2. **GPU Memory:** Better batch size auto-tuning for RTX 4080 Super
3. **GPU Chunking:** Reduce unnecessary batching (bottleneck identified)
4. **Code Architecture:** Refactor GPU to use core modules (future)

---

## ğŸ“Š Detailed Architecture Analysis

### Core Features Module (CPU)

**Location:** `ign_lidar/features/core/`

```
core/
â”œâ”€â”€ features.py âœ… (483 lines)
â”‚   â”œâ”€â”€ _compute_normals_and_eigenvalues_jit() - Numba JIT
â”‚   â”œâ”€â”€ _compute_all_features_jit() - Numba JIT
â”‚   â”œâ”€â”€ compute_normals() - 3-5Ã— faster
â”‚   â””â”€â”€ compute_all_features() - 5-8Ã— faster (single-pass)
â”‚
â”œâ”€â”€ normals.py âœ… (Standard fallback)
â”‚   â”œâ”€â”€ compute_normals() - scikit-learn based
â”‚   â””â”€â”€ compute_normals_fast/accurate() - Compatibility
â”‚
â”œâ”€â”€ curvature.py âœ…
â”œâ”€â”€ eigenvalues.py âœ…
â”œâ”€â”€ geometric.py âœ…
â”œâ”€â”€ architectural.py âœ…
â”œâ”€â”€ density.py âœ…
â””â”€â”€ unified.py âœ… (API dispatcher)
```

**Strengths:**

- âœ… Clean separation of concerns
- âœ… Numba JIT provides 3-8Ã— CPU speedup
- âœ… Single-pass computation (shared covariance)
- âœ… Fallback to standard numpy/scikit-learn
- âœ… Clear naming (no "unified/optimized" prefixes)

**Remaining Issues:**

- None - Core features are well-architected

---

### GPU Implementation (CuPy/cuML)

**Location:** `ign_lidar/features/`

#### A. GPU Single-Batch (`features_gpu.py` - 1,374 lines)

```python
class GPUFeatureComputer:
    def __init__(self, batch_size=8_000_000):
        # Auto-optimizes batch size based on VRAM
        if vram_gb >= 15.0:  # RTX 4080 Super
            self.batch_size = 12_000_000  # 50% increase
        elif vram_gb >= 12.0:
            self.batch_size = 6_000_000
        ...

    def compute_all_features(points, k=20):
        # Single-batch GPU processing
        # Uses CuPy + cuML for 15Ã— speedup
```

**Optimizations:**

- âœ… Adaptive batch sizing (RTX 4080: 12M points)
- âœ… Eigenvalue batching (500K matrices max per call)
- âœ… CuML NearestNeighbors for KNN
- âœ… CuPy for vectorized operations

**Strengths:**

- Fast for medium datasets (< 10M points)
- Simple single-batch logic
- Good VRAM utilization

**Issues:**

- Limited by single GPU memory allocation
- No chunking for very large datasets

---

#### B. GPU Chunked (`features_gpu_chunked.py` - 3,171 lines)

```python
class GPUChunkedFeatureComputer:
    def __init__(
        self,
        chunk_size=8_000_000,  # INCREASED for RTX 4080
        neighbor_query_batch_size=5_000_000,  # Controls chunking
        feature_batch_size=2_000_000  # Controls normal/curvature batching
    ):
        ...

    def compute_all_features_chunked(points, k=20):
        # Multi-chunk processing for large datasets
        # Week 1: 16Ã— optimization (353s â†’ 22s per chunk)
```

**Key Parameters:**

| Parameter                   | Default | RTX 4080 Optimal | Purpose                                |
| --------------------------- | ------- | ---------------- | -------------------------------------- |
| `chunk_size`                | 5M      | 8M               | Points per main chunk                  |
| `neighbor_query_batch_size` | 5M      | 30M+             | Controls neighbor query chunking       |
| `feature_batch_size`        | 2M      | 4M               | Controls normal/curvature batching     |
| `NEIGHBOR_BATCH_SIZE`       | 250K    | 500K             | Internal batch size (Week 1 optimized) |

**Week 1 Optimization:**

```python
# BEFORE (353s per chunk)
NEIGHBOR_BATCH_SIZE = 50_000  # Too small!

# AFTER (22s per chunk) âœ…
NEIGHBOR_BATCH_SIZE = 250_000  # Optimized for GPU L2 cache
```

**Current Bottleneck (Discovered in Analysis):**

```python
# Line 2745-2752: UNNECESSARY BATCHING
SAFE_BATCH_SIZE = 5_000_000  # Hardcoded!
if N > min_points_for_batching and num_query_batches == 1:
    batch_size = SAFE_BATCH_SIZE  # â† Ignores user's 30M config!
    num_query_batches = (N + batch_size - 1) // batch_size

# IMPACT:
# - 18.6M points â†’ Split into 4 batches (5M each)
# - User's neighbor_query_batch_size=30M IGNORED
# - 4Ã— slower neighbor queries than necessary
```

---

## ğŸš€ Optimization Roadmap

### Phase 3.0: GPU Restoration âœ… COMPLETE

**Status:** âœ… Done  
**Time:** 10 minutes  
**Result:** GPU files restored, imports working

**Changes:**

```bash
git restore ign_lidar/features/features_gpu.py
git restore ign_lidar/features/features_gpu_chunked.py
```

**Testing:**

```python
from ign_lidar.features.strategy_gpu import GPUStrategy  # âœ…
from ign_lidar.features.strategy_gpu_chunked import GPUChunkedStrategy  # âœ…
```

---

### Phase 3.1: Fix GPU Chunked Bottleneck ğŸ”¥ HIGH PRIORITY

**Goal:** Remove unnecessary batching in neighbor queries

**Status:** Not started  
**Time:** 30-60 minutes  
**Impact:** ~4Ã— faster neighbor queries for RTX 4080 Super

#### Problem Analysis

**File:** `features_gpu_chunked.py:2745-2769`

**Current Logic (BROKEN):**

```python
# Calculate memory estimate
estimated_memory_gb = (N * k * 8) / (1024**3)

# BUT THEN IGNORES IT!
SAFE_BATCH_SIZE = 5_000_000  # Hardcoded override
if N > self.min_points_for_batching and num_query_batches == 1:
    batch_size = SAFE_BATCH_SIZE  # â† USER CONFIG IGNORED!
    num_query_batches = (N + batch_size - 1) // batch_size
```

**Impact on 18.6M point dataset:**

- Memory needed: 18.6M Ã— 20 Ã— 8 = 2.98GB (perfectly safe on 16GB GPU!)
- Actual behavior: Split into 4 batches of 5M points
- Config value `neighbor_query_batch_size=30M` completely ignored
- Result: 4Ã— slower than necessary

#### Solution

**Replace lines 2745-2769 with smart logic:**

```python
def _should_batch_neighbor_queries(self, N, k, available_vram_gb):
    """
    Decide if neighbor queries need batching based on actual memory.

    Args:
        N: Number of points
        k: Number of neighbors
        available_vram_gb: Available GPU memory

    Returns:
        (should_batch, batch_size, num_batches)
    """
    # Calculate actual memory requirements
    # Neighbor indices: N Ã— k Ã— 4 bytes (int32)
    # Neighbor distances: N Ã— k Ã— 4 bytes (float32)
    estimated_memory_gb = (N * k * 8) / (1024**3)

    # Use 50% of available VRAM as threshold (conservative)
    memory_threshold_gb = available_vram_gb * 0.5

    if estimated_memory_gb <= memory_threshold_gb:
        # Memory is safe - NO BATCHING NEEDED!
        logger.info(
            f"âœ… Neighbor queries fit in VRAM: "
            f"{estimated_memory_gb:.2f}GB < {memory_threshold_gb:.2f}GB threshold"
        )
        return False, N, 1
    else:
        # Need batching - use USER'S configured batch size
        batch_size = self.neighbor_query_batch_size
        num_batches = (N + batch_size - 1) // batch_size
        logger.info(
            f"âš ï¸  Batching neighbor queries: "
            f"{estimated_memory_gb:.2f}GB > {memory_threshold_gb:.2f}GB threshold"
            f" â†’ {num_batches} batches of {batch_size:,} points"
        )
        return True, batch_size, num_batches


# In compute_all_features_chunked():
should_batch, batch_size, num_query_batches = self._should_batch_neighbor_queries(
    N, k, available_vram_gb
)

if should_batch:
    # Process in batches
    for batch_idx in range(num_query_batches):
        start = batch_idx * batch_size
        end = min(start + batch_size, N)
        # ... batch processing
else:
    # Single pass - FAST!
    distances, indices = knn_object.kneighbors(points_gpu, k)
```

**Expected Results:**

| Dataset   | Before (Batches) | After (Batches) | Speedup    |
| --------- | ---------------- | --------------- | ---------- |
| 18.6M pts | 4 Ã— 5M           | 1 Ã— 18.6M       | ~4Ã— faster |
| 5M pts    | 1 Ã— 5M           | 1 Ã— 5M          | No change  |
| 30M pts   | 6 Ã— 5M           | 1 Ã— 30M         | ~6Ã— faster |
| 50M pts   | 10 Ã— 5M          | 2 Ã— 25M         | ~5Ã— faster |

**Implementation Steps:**

1. Add `_should_batch_neighbor_queries()` method
2. Replace hardcoded `SAFE_BATCH_SIZE` logic
3. Respect user's `neighbor_query_batch_size` config
4. Add detailed logging
5. Test on 18.6M point dataset
6. Verify 4Ã— speedup

**Files to Modify:**

- `features_gpu_chunked.py` (lines 2745-2769)

**Estimated Time:** 30-60 minutes  
**Risk:** Low (well-tested logic, simple refactor)  
**Priority:** ğŸ”¥ HIGH - Immediate 4Ã— speedup available

---

### Phase 3.2: Optimize GPU Batch Sizes ğŸ“Š MEDIUM PRIORITY

**Goal:** Better auto-tuning for RTX 4080 Super

**Status:** Not started  
**Time:** 1-2 hours  
**Impact:** 10-20% throughput improvement

#### Current Batch Sizes

**GPU Single (`features_gpu.py`):**

```python
if vram_gb >= 15.0:  # RTX 4080 Super
    self.batch_size = 12_000_000  # âœ… Already optimized (50% increase)
```

**GPU Chunked (`features_gpu_chunked.py`):**

```python
# strategy_gpu_chunked.py
chunk_size = 8_000_000  # âœ… Already increased from 5M to 8M
batch_size = 500_000  # âœ… Already doubled from 250K to 500K

# features_gpu_chunked.py (internal)
neighbor_query_batch_size = 5_000_000  # âš ï¸  Too conservative for RTX 4080
feature_batch_size = 2_000_000  # âš ï¸  Could be 3-4M
NEIGHBOR_BATCH_SIZE = 250_000  # âœ… Week 1 optimized
```

#### Optimization Strategy

**A. Increase GPU Thresholds (DONE in GPU_BOTTLENECK_ANALYSIS.md)**

```python
# Current (lines 251-267)
elif vram_gb >= 14:
    threshold = 25_000_000  # âœ… Already 5Ã— increase (was 5M)
    gpu_tier = "Consumer (RTX 4080/3090)"
```

**B. Optimize Internal Batch Sizes**

```python
# For RTX 4080 Super (16GB VRAM):
DEFAULT_NEIGHBOR_QUERY_BATCH_SIZE = 20_000_000  # 4Ã— increase
DEFAULT_FEATURE_BATCH_SIZE = 4_000_000  # 2Ã— increase
DEFAULT_NEIGHBOR_BATCH_SIZE = 500_000  # 2Ã— increase from Week 1

# With memory-based auto-tuning:
if vram_gb >= 15.0:
    neighbor_query_batch_size = min(30_000_000, N)  # Process almost everything in 1 batch
    feature_batch_size = 4_000_000
    neighbor_batch_size = 500_000
elif vram_gb >= 12.0:
    neighbor_query_batch_size = 15_000_000
    feature_batch_size = 3_000_000
    neighbor_batch_size = 400_000
elif vram_gb >= 8.0:
    neighbor_query_batch_size = 10_000_000
    feature_batch_size = 2_000_000
    neighbor_batch_size = 300_000
```

**Expected Results:**

- RTX 4080: 10-20% faster throughput
- Better GPU utilization (fewer batches)
- Reduced kernel launch overhead

**Files to Modify:**

- `features_gpu_chunked.py` (lines 93-110, 411-448)

**Estimated Time:** 1-2 hours  
**Risk:** Low-Medium (test with various dataset sizes)  
**Priority:** ğŸ“Š MEDIUM

---

### Phase 3.3: Refactor GPU to Use Core Modules ğŸ—ï¸ LONG-TERM

**Goal:** Reduce code duplication between CPU and GPU

**Status:** Not started  
**Time:** 8-12 hours  
**Impact:** Maintainability, not performance

#### Current Duplication

**CPU (core/features.py):**

```python
@jit(nopython=True, parallel=True)
def _compute_normals_and_eigenvalues_jit(points, neighbor_indices, k):
    # Numba JIT-compiled CPU version
    for i in prange(n_points):
        # Compute covariance
        # Eigendecomposition
        ...
```

**GPU (features_gpu.py):**

```python
def compute_normals_gpu(points_gpu, k):
    # CuPy GPU version
    # DUPLICATES the algorithm, different implementation
    ...
```

#### Proposed Architecture

**Create GPU-specific core modules:**

```
core/
â”œâ”€â”€ features.py âœ… (CPU Numba)
â”œâ”€â”€ features_gpu.py ğŸ†• (GPU CuPy - lean version)
â”‚   â”œâ”€â”€ compute_normals_gpu()
â”‚   â”œâ”€â”€ compute_curvature_gpu()
â”‚   â””â”€â”€ compute_all_features_gpu()
â”‚
â”œâ”€â”€ normals.py âœ… (CPU standard)
â”œâ”€â”€ normals_gpu.py ğŸ†• (GPU CuPy)
â”‚
â”œâ”€â”€ curvature.py âœ… (CPU - already shared logic)
â”œâ”€â”€ geometric.py âœ… (CPU - already shared logic)
â”‚
â””â”€â”€ utils_gpu.py ğŸ†• (GPU utilities)
    â”œâ”€â”€ gpu_memory_info()
    â”œâ”€â”€ optimal_batch_size()
    â””â”€â”€ chunked_processing_helper()
```

**GPU Computer Classes (refactored):**

```
features/
â”œâ”€â”€ gpu_computer.py ğŸ†• (~400 lines, was 1,374)
â”‚   â””â”€â”€ GPUFeatureComputer (uses core/features_gpu.py)
â”‚
â””â”€â”€ gpu_chunked_computer.py ğŸ†• (~800 lines, was 3,171)
    â””â”€â”€ GPUChunkedFeatureComputer (uses core/features_gpu.py + chunking logic)
```

**Benefits:**

- âœ… 50%+ code reduction (3,872 â†’ ~1,800 lines)
- âœ… Shared algorithm logic (CPU and GPU use same high-level flow)
- âœ… Easier to maintain (one place to fix bugs)
- âœ… Clearer separation: core algorithms vs. orchestration

**Challenges:**

- âš ï¸ CuPy arrays â‰  NumPy arrays (different APIs)
- âš ï¸ GPU needs special memory management
- âš ï¸ Risk of breaking Week 1 optimizations

**Strategy:**

1. Extract GPU normals first (2-3 hours)
2. Test performance matches current
3. Extract GPU curvature (1-2 hours)
4. Extract GPU geometric features (2-3 hours)
5. Create lean GPU computer classes (2-3 hours)
6. Comprehensive testing (1-2 hours)

**Files to Create:**

- `core/features_gpu.py` (~300 lines)
- `core/normals_gpu.py` (~200 lines)
- `core/utils_gpu.py` (~100 lines)
- `features/gpu_computer.py` (~400 lines)
- `features/gpu_chunked_computer.py` (~800 lines)

**Files to Refactor:**

- `features_gpu.py` (1,374 â†’ 0 lines, code moved)
- `features_gpu_chunked.py` (3,171 â†’ 0 lines, code moved)
- `strategy_gpu.py` (import changes)
- `strategy_gpu_chunked.py` (import changes)

**Estimated Time:** 8-12 hours  
**Risk:** Medium (extensive testing needed)  
**Priority:** ğŸ—ï¸ LONG-TERM (do after Phase 3.1 and 3.2)

---

## ğŸ“‹ Immediate Action Items

### Today (30-60 minutes) ğŸ”¥

1. **Fix GPU Chunked Bottleneck (Phase 3.1)**
   - Implement `_should_batch_neighbor_queries()`
   - Remove hardcoded `SAFE_BATCH_SIZE`
   - Test on 18.6M point dataset
   - Verify 4Ã— speedup
   - Commit changes

### This Week (1-2 hours) ğŸ“Š

2. **Optimize GPU Batch Sizes (Phase 3.2)**
   - Increase RTX 4080 defaults
   - Test various dataset sizes
   - Measure throughput improvements
   - Update documentation

### Next Week (8-12 hours) ğŸ—ï¸

3. **Refactor GPU Architecture (Phase 3.3)**
   - Extract GPU normals to core
   - Create lean GPU computer classes
   - Maintain Week 1 performance
   - Comprehensive testing

---

## ğŸ¯ Success Metrics

### Phase 3.1 (Fix Bottleneck)

| Metric                   | Before    | Target  | Measured |
| ------------------------ | --------- | ------- | -------- |
| 18.6M pts neighbor query | 4 batches | 1 batch | â³ TBD   |
| Neighbor query time      | ~16s      | ~4s     | â³ TBD   |
| Total time (18.6M)       | ~60s      | ~48s    | â³ TBD   |
| Respects user config     | âŒ No     | âœ… Yes  | â³ TBD   |

### Phase 3.2 (Optimize Batching)

| Metric               | Before | Target | Measured |
| -------------------- | ------ | ------ | -------- |
| GPU utilization      | 70%    | 85%+   | â³ TBD   |
| Throughput (pts/sec) | 310K   | 350K+  | â³ TBD   |
| Batch overhead       | 20%    | 10%    | â³ TBD   |

### Phase 3.3 (Refactor)

| Metric           | Before      | Target       | Measured |
| ---------------- | ----------- | ------------ | -------- |
| GPU code size    | 3,872 lines | <2,000 lines | â³ TBD   |
| Code duplication | High        | Low          | â³ TBD   |
| Maintainability  | Medium      | High         | â³ TBD   |
| Performance      | 100%        | â‰¥100%        | â³ TBD   |

---

## ğŸ’¡ Key Principles

### For All GPU Optimizations

1. **âœ… Preserve Week 1 Gains:** Never regress from 16Ã— speedup
2. **âœ… Respect User Config:** Don't ignore batch size parameters
3. **âœ… Smart Memory Management:** Calculate actual needs, don't guess
4. **âœ… Progressive Enhancement:** Test at each step
5. **âœ… Measure Everything:** Before/after benchmarks required

### For Refactoring

1. **âœ… Different Hardware = Different Code:** CPU and GPU implementations should differ
2. **âœ… Share Algorithms, Not Code:** Same logic, different array types
3. **âœ… Specialize for Performance:** GPU needs chunking, batching, memory management
4. **âœ… Test Extensively:** Performance regressions are unacceptable

---

## ğŸ“š Related Documents

- `PHASE3_GPU_CRITICAL_ANALYSIS.md` - GPU restoration analysis
- `GPU_BOTTLENECK_ANALYSIS.md` - Neighbor query bottleneck
- `GPU_ADAPTIVE_BATCHING.md` - Batch size strategies
- `CORE_FEATURES_HARMONIZATION.md` - CPU core features cleanup
- `PHASE2_COMPLETE.md` - Feature consolidation (Phase 2)
- `NEXT_STEPS.md` - Overall project roadmap

---

**Status:** âœ… Ready for Phase 3.1  
**Priority:** ğŸ”¥ High - Immediate 4Ã— speedup available  
**Risk:** Low - Clear path, well-understood optimizations  
**Next:** Implement Phase 3.1 (30-60 minutes)

**Updated:** October 18, 2025  
**GPU:** RTX 4080 Super (16GB VRAM)
